{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json, gensim, sklearn, pickle, sys, re, os\n",
    "import IPython.display as ipd\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_short, strip_multiple_whitespaces, stem_text\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from IPython.display import Audio\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "sound_file = './Music/invalid_keypress.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapd=pd.read_json(\"arxiv-metadata-oai-snapshot.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapd_hep_th = datapd.loc[datapd['categories'].str.contains('hep-th')]\n",
    "hep_abstracts = datapd_hep_th[ list(datapd_hep_th.iloc[:,0:1]) + ['abstract'] + ['title']]\n",
    "hep_abstracts_limit=hep_abstracts[hep_abstracts.title.str.count(' ') < 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_corpus_limit=hep_abstracts_limit['abstract'].values \n",
    "hep_corpus_limit_eof=hep_corpus_limit+\" EOF\"\n",
    "short_hep_abs=hep_corpus_limit_eof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140500, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hep_abstracts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134296,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_hep_abs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_titles=hep_abstracts_limit['title'].values\n",
    "hep_titles_eof=hep_titles + \" EOF\"\n",
    "short_hep_tit=hep_titles_eof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By doing the above, we have obtained the 130K papers that belong in high energy theory physics (hep-th) ###\n",
    "\n",
    "### We restrict to papers that are not longer than 16 words. We also add an \" EOF\" to indicate the end of the abstract/title ###\n",
    "\n",
    "## We will now process the documents using gensim. This removes stop words, punctuations, capitalization and stems and lemmatizes the tezt ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokenized_nostop=preprocess_documents(short_hep_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctitle_tokenized_nostop=preprocess_documents(short_hep_tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "leabs = [len(x) for x in doc_tokenized_nostop]\n",
    "letit = [len(x) for x in doctitle_tokenized_nostop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for x in range(len(leabs)):\n",
    "    if leabs[x]>155:\n",
    "        count=count+1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for x in range(len(letit)):\n",
    "    if letit[x]>12:\n",
    "        count=count+1\n",
    "        #print(x)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above tells us that less than 0.5% of the papers have abstracts longer than 155 words or titles longer than 12 words. We will use this fact below when constructing our LSTM network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 199\n"
     ]
    }
   ],
   "source": [
    "print(max(letit),max(leabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134296 134296 134296\n"
     ]
    }
   ],
   "source": [
    "print(len(short_hep_abs),len(doctitle_tokenized_nostop),len(doc_tokenized_nostop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['argu',\n",
       " 'tachyon',\n",
       " 'free',\n",
       " 'type',\n",
       " 'string',\n",
       " 'vacua',\n",
       " 'supersymmetri',\n",
       " 'break',\n",
       " 'open',\n",
       " 'sector',\n",
       " 'string',\n",
       " 'scale',\n",
       " 'interpret',\n",
       " 'dualiti',\n",
       " 'argument',\n",
       " 'metast',\n",
       " 'vacua',\n",
       " 'supersymmetr',\n",
       " 'type',\n",
       " 'superstr',\n",
       " 'dynam',\n",
       " 'process',\n",
       " 'partial',\n",
       " 'captur',\n",
       " 'nucleat',\n",
       " 'brane',\n",
       " 'antibran',\n",
       " 'pair',\n",
       " 'non',\n",
       " 'supersymmetr',\n",
       " 'vacuum',\n",
       " 'subsequ',\n",
       " 'tachyon',\n",
       " 'condens',\n",
       " 'eof']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokenized_nostop[225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metast', 'string', 'vacua', 'eof']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctitle_tokenized_nostop[225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size=52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we train Word2Vec. This will learn word embeddings ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_hepmodel=Word2Vec(sentences=doc_tokenized_nostop,size=vec_size, window=5, min_count=1, workers=4, iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34969\n"
     ]
    }
   ],
   "source": [
    "totalvocab=len(short_hepmodel.wv.vocab)\n",
    "print(totalvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1417709, 1884064)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_hepmodel.build_vocab(doctitle_tokenized_nostop, update=True)\n",
    "short_hepmodel.train(doctitle_tokenized_nostop, total_examples=short_hepmodel.corpus_count, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35853\n"
     ]
    }
   ],
   "source": [
    "totalvocab=len(short_hepmodel.wv.vocab)\n",
    "print(totalvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(short_hepmodel,open(\"Modelsfeb18/hepmodel_feb_26_1558\", 'wb'))\n",
    "#short_hepmodel=pickle.load(open('ModelsFeb23/hepmodel_feb_26_1558','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising using t-SNE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,vec_size), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    close_words = model.wv.similar_by_word(word,topn=15)\n",
    "    \n",
    "\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 5), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()-15, x_coords.max()+15)\n",
    "    plt.ylim(y_coords.min()-15, y_coords.max()+15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsnescatterplot(short_hepmodel, 'newton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('proportion', 0.5625348091125488),\n",
       " ('torretti', 0.5618959069252014),\n",
       " ('newtonian', 0.5581762194633484),\n",
       " ('cosmolog', 0.5256727337837219),\n",
       " ('yearli', 0.5224730968475342),\n",
       " ('deviat', 0.522132396697998),\n",
       " ('finestructur', 0.5130105018615723),\n",
       " ('addititv', 0.500487744808197),\n",
       " ('modif', 0.4972865879535675),\n",
       " ('fick', 0.49547916650772095),\n",
       " ('malu', 0.4920443892478943),\n",
       " ('covariantli', 0.4866793155670166),\n",
       " ('maurer', 0.48058149218559265),\n",
       " ('supercovariantli', 0.47930726408958435),\n",
       " ('gstring', 0.47904014587402344)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_hepmodel.wv.most_similar(positive=[\"newton\"],topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will train an LSTM network with attention to get better word embeddings #\n",
    "\n",
    "## The aim of the task is to predict paper titles from the abstract. This is an impossibly hard task, but hopefully we will learn good word emebeddings along the way ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras  \n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting max length of the abstract and title to be 155 and 12 respectively. As shown above this is a reasonable thing to do ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx=155\n",
    "Ty=12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below function takes in sentences, and out puts a list of numbers. Each number correspond to a word. Thus the range of the numbers is from 0 to length of the vocab.  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, hepmodel,max_len):\n",
    "    m = X.shape[0] \n",
    "    X_indices = np.zeros((m,max_len),dtype='uint16')+hepmodel.wv.vocab.get('eof').index\n",
    "    for i in range(m):                          \n",
    "        sentence_words = X[i]\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            X_indices[i, j] = hepmodel.wv.vocab.get(w).index\n",
    "            j = j+1\n",
    "            if j>=max_len:\n",
    "                break\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "array_abs=np.asarray(doc_tokenized_nostop)\n",
    "array_tit=np.asarray(doctitle_tokenized_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X_indices=sentences_to_indices(array_abs,short_hepmodel,Tx)\n",
    "train_Y_indices=sentences_to_indices(array_tit,short_hepmodel,Ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below function creates a small dense network for the attention layer ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import softmax\n",
    "def softmax_axis1(x):\n",
    "    return softmax(x,axis=1)\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(Tx)\n",
    "densor2 = Dense(Tx)\n",
    "densor3 = Dense(10)\n",
    "densor4 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax_axis1, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    s_prev = repeator(s_prev)\n",
    "    concat = concatenator([a,s_prev])\n",
    "\n",
    "    e = densor1(concat)\n",
    "    e = tf.keras.layers.LeakyReLU(alpha=0.3)(e)\n",
    "\n",
    "\n",
    "    e = densor3(e)\n",
    "    e = tf.keras.layers.LeakyReLU(alpha=0.3)(e)\n",
    "\n",
    "    energies = densor4(e)\n",
    "    alphas = activator(energies)\n",
    "    context = dotor([alphas,a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = [['witten' 'unif' 'string' 'theori']\n",
      " ['dirac' 'discover' 'field' 'theori']]\n",
      "X1_indices =\n",
      " [[  426   969    10     0     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1]\n",
      " [  236 13827     2     0     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([['witten', 'unif', 'string', 'theori' ],['dirac', 'discover', 'field' , 'theori']])\n",
    "X1_indices = sentences_to_indices(X1, short_hepmodel,max_len = Tx)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's set up the LSTM network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True) \n",
    "output_layer = Dense(totalvocab, activation=softmax_axis1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We initialize the vectors to be small random vectors. We *Do Not* initialize it with the Word2Vec vectors that we obtained above ##\n",
    "\n",
    "## We found that when initialized with Word2Vec the LSTM training changes the word vectors by less than 5%. In other words the vectors are stuck near a local minima.  ##\n",
    "\n",
    "## If we initialize them randomly. The final vectors are very different from the initialized random vectors, and subsequently these new final vectors encode meaning differently ##\n",
    "\n",
    "# As mentioned in the report, Word2Vec is good at encoding words that are related *and* that appear together. While LSTM is good at encoding words that are related *but* do not appear together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_pre=(2*np.random.rand(totalvocab, vec_size)-1)*0.01\n",
    "small_vectors=small_pre*short_hepmodel.wv.vectors\n",
    "appker=np.zeros((1,vec_size))\n",
    "kerasvectors=np.vstack((small_vectors,appker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the embedding layer and set *trainable=True*. We will read the weights from this layer once the network has trained. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(hepmodel):\n",
    "    vocab_len = totalvocab + 1                  \n",
    "    emb_dim = hepmodel.wv[\"newton\"].shape[0]      \n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    embedding_layer = Embedding(vocab_len,emb_dim,trainable=True)\n",
    "    embedding_layer.build((None,)) \n",
    "    embedding_layer.set_weights([kerasvectors])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1628][12] = -9.339144e-05\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(short_hepmodel)\n",
    "print(\"weights[0][1628][12] =\", embedding_layer.get_weights()[0][1628][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  83,  491,   63, ...,    1,    1,    1],\n",
       "       [4041,    4, 8213, ...,    1,    1,    1],\n",
       "       [   6,    2,    0, ...,    1,    1,    1],\n",
       "       ...,\n",
       "       [3399,  955,  541, ...,    1,    1,    1],\n",
       "       [ 158, 3117,  541, ...,   91,    1,    1],\n",
       "       [  10,    3, 2707, ...,    1,    1,    1]], dtype=uint16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model. Two bi-directional LSTMs --> Attention layer ---> LSTM ----> Softmax. The output is (Ty) number of column vectors of dimension (totalvocab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerasmagic(input_shape, Tx, Ty, n_a, n_s, totalvocab, hepmodel): \n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer \n",
    "    embedding_layer = pretrained_embedding_layer(hepmodel)\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    \n",
    "    a = Bidirectional(LSTM(units=n_a, return_sequences=True))(embeddings)\n",
    "    b = Bidirectional(LSTM(units=n_a, return_sequences=True))(a)\n",
    "    \n",
    "    #b = Bidirectional(LSTM(units=n_a))(b)\n",
    "        \n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Perform one step of the attention mechanism #\n",
    "        context = one_step_attention(b,s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        s, _ , c = post_activation_LSTM_cell(context, initial_state=[s,c])\n",
    "        out = output_layer(s)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    model = Model([sentence_indices,s0,c0],outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasmodel=kerasmagic(Tx,Tx,Ty,n_a,n_s,totalvocab,short_hepmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 155)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 155, 52)      1864408     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 155, 128)     59904       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 155, 128)     98816       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 155, 64)      0           s0[0][0]                         \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "                                                                 lstm[10][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 155, 192)     0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[0][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[1][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[2][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[3][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[4][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[5][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[6][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[7][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[8][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[9][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[10][0]             \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector[11][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 155, 155)     29915       concatenate[0][0]                \n",
      "                                                                 concatenate[1][0]                \n",
      "                                                                 concatenate[2][0]                \n",
      "                                                                 concatenate[3][0]                \n",
      "                                                                 concatenate[4][0]                \n",
      "                                                                 concatenate[5][0]                \n",
      "                                                                 concatenate[6][0]                \n",
      "                                                                 concatenate[7][0]                \n",
      "                                                                 concatenate[8][0]                \n",
      "                                                                 concatenate[9][0]                \n",
      "                                                                 concatenate[10][0]               \n",
      "                                                                 concatenate[11][0]               \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 155, 155)     0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 155, 10)      1560        leaky_re_lu[0][0]                \n",
      "                                                                 leaky_re_lu_2[0][0]              \n",
      "                                                                 leaky_re_lu_4[0][0]              \n",
      "                                                                 leaky_re_lu_6[0][0]              \n",
      "                                                                 leaky_re_lu_8[0][0]              \n",
      "                                                                 leaky_re_lu_10[0][0]             \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "                                                                 leaky_re_lu_14[0][0]             \n",
      "                                                                 leaky_re_lu_16[0][0]             \n",
      "                                                                 leaky_re_lu_18[0][0]             \n",
      "                                                                 leaky_re_lu_20[0][0]             \n",
      "                                                                 leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 155, 10)      0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 155, 1)       11          leaky_re_lu_1[0][0]              \n",
      "                                                                 leaky_re_lu_3[0][0]              \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "                                                                 leaky_re_lu_7[0][0]              \n",
      "                                                                 leaky_re_lu_9[0][0]              \n",
      "                                                                 leaky_re_lu_11[0][0]             \n",
      "                                                                 leaky_re_lu_13[0][0]             \n",
      "                                                                 leaky_re_lu_15[0][0]             \n",
      "                                                                 leaky_re_lu_17[0][0]             \n",
      "                                                                 leaky_re_lu_19[0][0]             \n",
      "                                                                 leaky_re_lu_21[0][0]             \n",
      "                                                                 leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 155, 1)       0           dense_3[0][0]                    \n",
      "                                                                 dense_3[1][0]                    \n",
      "                                                                 dense_3[2][0]                    \n",
      "                                                                 dense_3[3][0]                    \n",
      "                                                                 dense_3[4][0]                    \n",
      "                                                                 dense_3[5][0]                    \n",
      "                                                                 dense_3[6][0]                    \n",
      "                                                                 dense_3[7][0]                    \n",
      "                                                                 dense_3[8][0]                    \n",
      "                                                                 dense_3[9][0]                    \n",
      "                                                                 dense_3[10][0]                   \n",
      "                                                                 dense_3[11][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[10][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[11][0]         \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 64), (None,  49408       dot[0][0]                        \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot[1][0]                        \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "                                                                 dot[2][0]                        \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[1][2]                       \n",
      "                                                                 dot[3][0]                        \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[2][2]                       \n",
      "                                                                 dot[4][0]                        \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[3][2]                       \n",
      "                                                                 dot[5][0]                        \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[4][2]                       \n",
      "                                                                 dot[6][0]                        \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[5][2]                       \n",
      "                                                                 dot[7][0]                        \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[6][2]                       \n",
      "                                                                 dot[8][0]                        \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[7][2]                       \n",
      "                                                                 dot[9][0]                        \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[8][2]                       \n",
      "                                                                 dot[10][0]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "                                                                 lstm[9][2]                       \n",
      "                                                                 dot[11][0]                       \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 lstm[10][2]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 155, 155)     0           dense[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 155, 10)      0           dense_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 155, 155)     0           dense[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 155, 10)      0           dense_2[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 155, 155)     0           dense[3][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 155, 10)      0           dense_2[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 155, 155)     0           dense[4][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 155, 10)      0           dense_2[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 155, 155)     0           dense[5][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 155, 10)      0           dense_2[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 155, 155)     0           dense[6][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 155, 10)      0           dense_2[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 155, 155)     0           dense[7][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 155, 10)      0           dense_2[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 155, 155)     0           dense[8][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 155, 10)      0           dense_2[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 155, 155)     0           dense[9][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 155, 10)      0           dense_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 155, 155)     0           dense[10][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 155, 10)      0           dense_2[10][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 155, 155)     0           dense[11][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 155, 10)      0           dense_2[11][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 35853)        2330445     lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 lstm[11][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,434,467\n",
      "Trainable params: 4,434,467\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kerasmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
    "kerasmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theori'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_hepmodel.wv.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41631760\n",
      "3223104\n"
     ]
    }
   ],
   "source": [
    "print(train_X_indices.size*train_X_indices.itemsize)\n",
    "print(train_Y_indices.size*train_Y_indices.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### echo 1 | sudo tee /proc/sys/vm/overcommit_memory ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = totalvocab\n",
    "text_length = len(train_Y_indices[0])\n",
    "one_hot = np.zeros([train_X_indices.shape[0],vocab_size,text_length],dtype='uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_X_indices.shape[0]):\n",
    "    for j in range(Ty):\n",
    "        one_hot[i,train_Y_indices[i,j],j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "listhot = list(one_hot.T.swapaxes(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134296, 155)\n",
      "(134296, 35853)\n"
     ]
    }
   ],
   "source": [
    "one_hot.shape\n",
    "print(train_X_indices.shape)\n",
    "print(listhot[7].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchGenerator(train_X_indices,one_hot):\n",
    "    for i in range(1,33):\n",
    "        X_train = train_X_indices[4096*(i-1):4096*i,:]\n",
    "        one_hot_sliced = one_hot[4096*(i-1):4096*i,:,:]\n",
    "        Y_train = list(one_hot_sliced.T.swapaxes(1,2))\n",
    "        yield (X_train, Y_train)\n",
    "    X_train = train_X_indices[4096*i:-1,:]\n",
    "    one_hot_sliced = one_hot[4096*i:-1,:,:]\n",
    "    Y_train = list(one_hot_sliced.T.swapaxes(1,2))\n",
    "    yield (X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Metabatch number --------------------- 1 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 48.5081 - dense_4_loss: 7.3711 - dense_4_1_loss: 7.6120 - dense_4_2_loss: 7.4299 - dense_4_3_loss: 6.8185 - dense_4_4_loss: 5.7455 - dense_4_5_loss: 4.4640 - dense_4_6_loss: 3.1941 - dense_4_7_loss: 2.1088 - dense_4_8_loss: 1.4206 - dense_4_9_loss: 0.9324 - dense_4_10_loss: 0.7395 - dense_4_11_loss: 0.6717 - dense_4_accuracy: 4.8828e-04 - dense_4_1_accuracy: 0.0022 - dense_4_2_accuracy: 0.0269 - dense_4_3_accuracy: 0.0957 - dense_4_4_accuracy: 0.2344 - dense_4_5_accuracy: 0.4246 - dense_4_6_accuracy: 0.6094 - dense_4_7_accuracy: 0.7725 - dense_4_8_accuracy: 0.8774 - dense_4_9_accuracy: 0.9500 - dense_4_10_accuracy: 0.9819 - dense_4_11_accuracy: 0.9929\n",
      " \n",
      " Metabatch number --------------------- 2 \n",
      " \n",
      "32/32 [==============================] - 16s 486ms/step - loss: 48.5013 - dense_4_loss: 7.4147 - dense_4_1_loss: 7.6224 - dense_4_2_loss: 7.4754 - dense_4_3_loss: 6.9260 - dense_4_4_loss: 5.8208 - dense_4_5_loss: 4.4197 - dense_4_6_loss: 3.1150 - dense_4_7_loss: 2.0691 - dense_4_8_loss: 1.3432 - dense_4_9_loss: 0.9193 - dense_4_10_loss: 0.7338 - dense_4_11_loss: 0.6418 - dense_4_accuracy: 0.0012 - dense_4_1_accuracy: 0.0022 - dense_4_2_accuracy: 0.0256 - dense_4_3_accuracy: 0.0911 - dense_4_4_accuracy: 0.2395 - dense_4_5_accuracy: 0.4324 - dense_4_6_accuracy: 0.6243 - dense_4_7_accuracy: 0.7820 - dense_4_8_accuracy: 0.8909 - dense_4_9_accuracy: 0.9526 - dense_4_10_accuracy: 0.9810 - dense_4_11_accuracy: 0.9956\n",
      " \n",
      " Metabatch number --------------------- 3 \n",
      " \n",
      "32/32 [==============================] - 16s 487ms/step - loss: 49.5060 - dense_4_loss: 7.5428 - dense_4_1_loss: 7.7632 - dense_4_2_loss: 7.5087 - dense_4_3_loss: 6.9755 - dense_4_4_loss: 5.9095 - dense_4_5_loss: 4.5890 - dense_4_6_loss: 3.2856 - dense_4_7_loss: 2.1478 - dense_4_8_loss: 1.4283 - dense_4_9_loss: 0.9805 - dense_4_10_loss: 0.7230 - dense_4_11_loss: 0.6519 - dense_4_accuracy: 2.4414e-04 - dense_4_1_accuracy: 0.0037 - dense_4_2_accuracy: 0.0254 - dense_4_3_accuracy: 0.0959 - dense_4_4_accuracy: 0.2307 - dense_4_5_accuracy: 0.4187 - dense_4_6_accuracy: 0.6030 - dense_4_7_accuracy: 0.7705 - dense_4_8_accuracy: 0.8787 - dense_4_9_accuracy: 0.9458 - dense_4_10_accuracy: 0.9841 - dense_4_11_accuracy: 0.9951\n",
      " \n",
      " Metabatch number --------------------- 4 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 50.0259 - dense_4_loss: 7.5870 - dense_4_1_loss: 7.8061 - dense_4_2_loss: 7.6606 - dense_4_3_loss: 7.0000 - dense_4_4_loss: 5.9864 - dense_4_5_loss: 4.6391 - dense_4_6_loss: 3.2867 - dense_4_7_loss: 2.2257 - dense_4_8_loss: 1.4481 - dense_4_9_loss: 0.9804 - dense_4_10_loss: 0.7567 - dense_4_11_loss: 0.6493 - dense_4_accuracy: 2.4414e-04 - dense_4_1_accuracy: 0.0024 - dense_4_2_accuracy: 0.0234 - dense_4_3_accuracy: 0.0935 - dense_4_4_accuracy: 0.2236 - dense_4_5_accuracy: 0.4106 - dense_4_6_accuracy: 0.6042 - dense_4_7_accuracy: 0.7676 - dense_4_8_accuracy: 0.8770 - dense_4_9_accuracy: 0.9475 - dense_4_10_accuracy: 0.9814 - dense_4_11_accuracy: 0.9968\n",
      " \n",
      " Metabatch number --------------------- 5 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 49.5239 - dense_4_loss: 7.4692 - dense_4_1_loss: 7.7183 - dense_4_2_loss: 7.5506 - dense_4_3_loss: 7.0851 - dense_4_4_loss: 5.9040 - dense_4_5_loss: 4.5478 - dense_4_6_loss: 3.2567 - dense_4_7_loss: 2.1910 - dense_4_8_loss: 1.4285 - dense_4_9_loss: 0.9841 - dense_4_10_loss: 0.7363 - dense_4_11_loss: 0.6524 - dense_4_accuracy: 0.0000e+00 - dense_4_1_accuracy: 0.0029 - dense_4_2_accuracy: 0.0225 - dense_4_3_accuracy: 0.0898 - dense_4_4_accuracy: 0.2444 - dense_4_5_accuracy: 0.4263 - dense_4_6_accuracy: 0.6135 - dense_4_7_accuracy: 0.7673 - dense_4_8_accuracy: 0.8794 - dense_4_9_accuracy: 0.9456 - dense_4_10_accuracy: 0.9814 - dense_4_11_accuracy: 0.9954\n",
      " \n",
      " Metabatch number --------------------- 6 \n",
      " \n",
      "32/32 [==============================] - 16s 489ms/step - loss: 49.5976 - dense_4_loss: 7.4960 - dense_4_1_loss: 7.7343 - dense_4_2_loss: 7.6108 - dense_4_3_loss: 6.9835 - dense_4_4_loss: 5.8715 - dense_4_5_loss: 4.5495 - dense_4_6_loss: 3.2929 - dense_4_7_loss: 2.2657 - dense_4_8_loss: 1.4521 - dense_4_9_loss: 0.9625 - dense_4_10_loss: 0.7198 - dense_4_11_loss: 0.6590 - dense_4_accuracy: 2.4414e-04 - dense_4_1_accuracy: 0.0022 - dense_4_2_accuracy: 0.0244 - dense_4_3_accuracy: 0.0964 - dense_4_4_accuracy: 0.2419 - dense_4_5_accuracy: 0.4236 - dense_4_6_accuracy: 0.6062 - dense_4_7_accuracy: 0.7603 - dense_4_8_accuracy: 0.8755 - dense_4_9_accuracy: 0.9475 - dense_4_10_accuracy: 0.9846 - dense_4_11_accuracy: 0.9941\n",
      " \n",
      " Metabatch number --------------------- 7 \n",
      " \n",
      "32/32 [==============================] - 16s 487ms/step - loss: 49.8196 - dense_4_loss: 7.5028 - dense_4_1_loss: 7.7328 - dense_4_2_loss: 7.5630 - dense_4_3_loss: 7.0205 - dense_4_4_loss: 5.8953 - dense_4_5_loss: 4.6364 - dense_4_6_loss: 3.3225 - dense_4_7_loss: 2.1830 - dense_4_8_loss: 1.4860 - dense_4_9_loss: 1.0146 - dense_4_10_loss: 0.7915 - dense_4_11_loss: 0.6713 - dense_4_accuracy: 0.0000e+00 - dense_4_1_accuracy: 0.0027 - dense_4_2_accuracy: 0.0225 - dense_4_3_accuracy: 0.0945 - dense_4_4_accuracy: 0.2393 - dense_4_5_accuracy: 0.4177 - dense_4_6_accuracy: 0.6086 - dense_4_7_accuracy: 0.7659 - dense_4_8_accuracy: 0.8728 - dense_4_9_accuracy: 0.9434 - dense_4_10_accuracy: 0.9756 - dense_4_11_accuracy: 0.9924\n",
      " \n",
      " Metabatch number --------------------- 8 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 49.8505 - dense_4_loss: 7.4616 - dense_4_1_loss: 7.7039 - dense_4_2_loss: 7.5295 - dense_4_3_loss: 6.9783 - dense_4_4_loss: 5.9185 - dense_4_5_loss: 4.6551 - dense_4_6_loss: 3.3438 - dense_4_7_loss: 2.2490 - dense_4_8_loss: 1.5425 - dense_4_9_loss: 1.0079 - dense_4_10_loss: 0.7797 - dense_4_11_loss: 0.6808 - dense_4_accuracy: 0.0000e+00 - dense_4_1_accuracy: 0.0020 - dense_4_2_accuracy: 0.0242 - dense_4_3_accuracy: 0.0942 - dense_4_4_accuracy: 0.2329 - dense_4_5_accuracy: 0.4133 - dense_4_6_accuracy: 0.5950 - dense_4_7_accuracy: 0.7583 - dense_4_8_accuracy: 0.8684 - dense_4_9_accuracy: 0.9424 - dense_4_10_accuracy: 0.9775 - dense_4_11_accuracy: 0.9924\n",
      " \n",
      " Metabatch number --------------------- 9 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 50.4871 - dense_4_loss: 7.4673 - dense_4_1_loss: 7.7464 - dense_4_2_loss: 7.5791 - dense_4_3_loss: 7.0413 - dense_4_4_loss: 6.1505 - dense_4_5_loss: 4.7161 - dense_4_6_loss: 3.4334 - dense_4_7_loss: 2.3068 - dense_4_8_loss: 1.5156 - dense_4_9_loss: 1.0489 - dense_4_10_loss: 0.7970 - dense_4_11_loss: 0.6846 - dense_4_accuracy: 0.0000e+00 - dense_4_1_accuracy: 0.0022 - dense_4_2_accuracy: 0.0220 - dense_4_3_accuracy: 0.0886 - dense_4_4_accuracy: 0.2126 - dense_4_5_accuracy: 0.4038 - dense_4_6_accuracy: 0.5947 - dense_4_7_accuracy: 0.7561 - dense_4_8_accuracy: 0.8696 - dense_4_9_accuracy: 0.9390 - dense_4_10_accuracy: 0.9756 - dense_4_11_accuracy: 0.9915\n",
      " \n",
      " Metabatch number --------------------- 10 \n",
      " \n",
      "32/32 [==============================] - 16s 489ms/step - loss: 50.0043 - dense_4_loss: 7.5043 - dense_4_1_loss: 7.7171 - dense_4_2_loss: 7.6466 - dense_4_3_loss: 7.0234 - dense_4_4_loss: 5.9140 - dense_4_5_loss: 4.6249 - dense_4_6_loss: 3.3357 - dense_4_7_loss: 2.2413 - dense_4_8_loss: 1.4981 - dense_4_9_loss: 1.0334 - dense_4_10_loss: 0.7925 - dense_4_11_loss: 0.6730 - dense_4_accuracy: 0.0000e+00 - dense_4_1_accuracy: 0.0029 - dense_4_2_accuracy: 0.0222 - dense_4_3_accuracy: 0.0903 - dense_4_4_accuracy: 0.2366 - dense_4_5_accuracy: 0.4133 - dense_4_6_accuracy: 0.6050 - dense_4_7_accuracy: 0.7651 - dense_4_8_accuracy: 0.8757 - dense_4_9_accuracy: 0.9404 - dense_4_10_accuracy: 0.9768 - dense_4_11_accuracy: 0.9932\n",
      " \n",
      " Metabatch number --------------------- 11 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 49.9916 - dense_4_loss: 7.4945 - dense_4_1_loss: 7.6951 - dense_4_2_loss: 7.5322 - dense_4_3_loss: 7.0917 - dense_4_4_loss: 6.0747 - dense_4_5_loss: 4.6514 - dense_4_6_loss: 3.3208 - dense_4_7_loss: 2.2592 - dense_4_8_loss: 1.4635 - dense_4_9_loss: 0.9875 - dense_4_10_loss: 0.7604 - dense_4_11_loss: 0.6607 - dense_4_accuracy: 0.0000e+00 - dense_4_1_accuracy: 0.0020 - dense_4_2_accuracy: 0.0244 - dense_4_3_accuracy: 0.0823 - dense_4_4_accuracy: 0.2197 - dense_4_5_accuracy: 0.4177 - dense_4_6_accuracy: 0.5994 - dense_4_7_accuracy: 0.7573 - dense_4_8_accuracy: 0.8733 - dense_4_9_accuracy: 0.9470 - dense_4_10_accuracy: 0.9797 - dense_4_11_accuracy: 0.9939\n",
      " \n",
      " Metabatch number --------------------- 12 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 50.3225 - dense_4_loss: 7.5033 - dense_4_1_loss: 7.7814 - dense_4_2_loss: 7.6070 - dense_4_3_loss: 7.0240 - dense_4_4_loss: 6.0253 - dense_4_5_loss: 4.6815 - dense_4_6_loss: 3.4090 - dense_4_7_loss: 2.2601 - dense_4_8_loss: 1.4722 - dense_4_9_loss: 1.0737 - dense_4_10_loss: 0.7970 - dense_4_11_loss: 0.6881 - dense_4_accuracy: 2.4414e-04 - dense_4_1_accuracy: 0.0017 - dense_4_2_accuracy: 0.0237 - dense_4_3_accuracy: 0.0859 - dense_4_4_accuracy: 0.2285 - dense_4_5_accuracy: 0.4075 - dense_4_6_accuracy: 0.5869 - dense_4_7_accuracy: 0.7576 - dense_4_8_accuracy: 0.8752 - dense_4_9_accuracy: 0.9368 - dense_4_10_accuracy: 0.9775 - dense_4_11_accuracy: 0.9929\n",
      " \n",
      " Metabatch number --------------------- 13 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 50.5438 - dense_4_loss: 7.4629 - dense_4_1_loss: 7.8019 - dense_4_2_loss: 7.6042 - dense_4_3_loss: 7.0765 - dense_4_4_loss: 6.0775 - dense_4_5_loss: 4.6779 - dense_4_6_loss: 3.4479 - dense_4_7_loss: 2.3651 - dense_4_8_loss: 1.5163 - dense_4_9_loss: 1.0542 - dense_4_10_loss: 0.7802 - dense_4_11_loss: 0.6792 - dense_4_accuracy: 4.8828e-04 - dense_4_1_accuracy: 0.0020 - dense_4_2_accuracy: 0.0227 - dense_4_3_accuracy: 0.0916 - dense_4_4_accuracy: 0.2231 - dense_4_5_accuracy: 0.4099 - dense_4_6_accuracy: 0.5920 - dense_4_7_accuracy: 0.7454 - dense_4_8_accuracy: 0.8672 - dense_4_9_accuracy: 0.9397 - dense_4_10_accuracy: 0.9775 - dense_4_11_accuracy: 0.9922\n",
      " \n",
      " Metabatch number --------------------- 14 \n",
      " \n",
      "32/32 [==============================] - 16s 487ms/step - loss: 50.7301 - dense_4_loss: 7.5074 - dense_4_1_loss: 7.6864 - dense_4_2_loss: 7.5374 - dense_4_3_loss: 7.1502 - dense_4_4_loss: 6.1008 - dense_4_5_loss: 4.8189 - dense_4_6_loss: 3.5305 - dense_4_7_loss: 2.3967 - dense_4_8_loss: 1.5348 - dense_4_9_loss: 1.0480 - dense_4_10_loss: 0.7655 - dense_4_11_loss: 0.6535 - dense_4_accuracy: 4.8828e-04 - dense_4_1_accuracy: 0.0017 - dense_4_2_accuracy: 0.0217 - dense_4_3_accuracy: 0.0796 - dense_4_4_accuracy: 0.2144 - dense_4_5_accuracy: 0.3906 - dense_4_6_accuracy: 0.5771 - dense_4_7_accuracy: 0.7380 - dense_4_8_accuracy: 0.8652 - dense_4_9_accuracy: 0.9351 - dense_4_10_accuracy: 0.9746 - dense_4_11_accuracy: 0.9917\n",
      " \n",
      " Metabatch number --------------------- 15 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 50.2039 - dense_4_loss: 7.4350 - dense_4_1_loss: 7.6450 - dense_4_2_loss: 7.4869 - dense_4_3_loss: 7.0415 - dense_4_4_loss: 6.0202 - dense_4_5_loss: 4.8684 - dense_4_6_loss: 3.5029 - dense_4_7_loss: 2.3275 - dense_4_8_loss: 1.5288 - dense_4_9_loss: 1.0266 - dense_4_10_loss: 0.7230 - dense_4_11_loss: 0.5981 - dense_4_accuracy: 0.0000e+00 - dense_4_1_accuracy: 0.0015 - dense_4_2_accuracy: 0.0222 - dense_4_3_accuracy: 0.0803 - dense_4_4_accuracy: 0.2146 - dense_4_5_accuracy: 0.3894 - dense_4_6_accuracy: 0.5776 - dense_4_7_accuracy: 0.7463 - dense_4_8_accuracy: 0.8635 - dense_4_9_accuracy: 0.9287 - dense_4_10_accuracy: 0.9734 - dense_4_11_accuracy: 0.9900\n",
      " \n",
      " Metabatch number --------------------- 16 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 49.6140 - dense_4_loss: 7.3675 - dense_4_1_loss: 7.5200 - dense_4_2_loss: 7.4109 - dense_4_3_loss: 6.9510 - dense_4_4_loss: 6.1220 - dense_4_5_loss: 4.8498 - dense_4_6_loss: 3.5778 - dense_4_7_loss: 2.3794 - dense_4_8_loss: 1.4818 - dense_4_9_loss: 0.8829 - dense_4_10_loss: 0.6017 - dense_4_11_loss: 0.4692 - dense_4_accuracy: 0.0000e+00 - dense_4_1_accuracy: 0.0017 - dense_4_2_accuracy: 0.0178 - dense_4_3_accuracy: 0.0820 - dense_4_4_accuracy: 0.2144 - dense_4_5_accuracy: 0.3911 - dense_4_6_accuracy: 0.5742 - dense_4_7_accuracy: 0.7375 - dense_4_8_accuracy: 0.8586 - dense_4_9_accuracy: 0.9380 - dense_4_10_accuracy: 0.9746 - dense_4_11_accuracy: 0.9932\n",
      " \n",
      " Metabatch number --------------------- 17 \n",
      " \n",
      "32/32 [==============================] - 16s 489ms/step - loss: 49.2598 - dense_4_loss: 7.3211 - dense_4_1_loss: 7.3927 - dense_4_2_loss: 7.2591 - dense_4_3_loss: 6.8274 - dense_4_4_loss: 6.1344 - dense_4_5_loss: 4.9166 - dense_4_6_loss: 3.6226 - dense_4_7_loss: 2.3872 - dense_4_8_loss: 1.4857 - dense_4_9_loss: 0.8900 - dense_4_10_loss: 0.5858 - dense_4_11_loss: 0.4373 - dense_4_accuracy: 0.0000e+00 - dense_4_1_accuracy: 7.3242e-04 - dense_4_2_accuracy: 0.0208 - dense_4_3_accuracy: 0.0813 - dense_4_4_accuracy: 0.2029 - dense_4_5_accuracy: 0.3867 - dense_4_6_accuracy: 0.5725 - dense_4_7_accuracy: 0.7349 - dense_4_8_accuracy: 0.8552 - dense_4_9_accuracy: 0.9324 - dense_4_10_accuracy: 0.9709 - dense_4_11_accuracy: 0.9905\n",
      " \n",
      " Metabatch number --------------------- 18 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 49.2017 - dense_4_loss: 7.3422 - dense_4_1_loss: 7.3842 - dense_4_2_loss: 7.2554 - dense_4_3_loss: 6.9557 - dense_4_4_loss: 6.1107 - dense_4_5_loss: 4.8765 - dense_4_6_loss: 3.6284 - dense_4_7_loss: 2.4471 - dense_4_8_loss: 1.5013 - dense_4_9_loss: 0.8537 - dense_4_10_loss: 0.5085 - dense_4_11_loss: 0.3381 - dense_4_accuracy: 2.4414e-04 - dense_4_1_accuracy: 0.0022 - dense_4_2_accuracy: 0.0171 - dense_4_3_accuracy: 0.0762 - dense_4_4_accuracy: 0.2004 - dense_4_5_accuracy: 0.3804 - dense_4_6_accuracy: 0.5662 - dense_4_7_accuracy: 0.7253 - dense_4_8_accuracy: 0.8477 - dense_4_9_accuracy: 0.9304 - dense_4_10_accuracy: 0.9714 - dense_4_11_accuracy: 0.9924\n",
      " \n",
      " Metabatch number --------------------- 19 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 49.3179 - dense_4_loss: 7.3682 - dense_4_1_loss: 7.2967 - dense_4_2_loss: 7.1032 - dense_4_3_loss: 6.8449 - dense_4_4_loss: 6.1812 - dense_4_5_loss: 5.1120 - dense_4_6_loss: 3.8026 - dense_4_7_loss: 2.4888 - dense_4_8_loss: 1.4951 - dense_4_9_loss: 0.8391 - dense_4_10_loss: 0.4799 - dense_4_11_loss: 0.3062 - dense_4_accuracy: 0.0000e+00 - dense_4_1_accuracy: 0.0022 - dense_4_2_accuracy: 0.0188 - dense_4_3_accuracy: 0.0745 - dense_4_4_accuracy: 0.1931 - dense_4_5_accuracy: 0.3635 - dense_4_6_accuracy: 0.5496 - dense_4_7_accuracy: 0.7188 - dense_4_8_accuracy: 0.8459 - dense_4_9_accuracy: 0.9250 - dense_4_10_accuracy: 0.9700 - dense_4_11_accuracy: 0.9915\n",
      " \n",
      " Metabatch number --------------------- 20 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 46.6608 - dense_4_loss: 7.1708 - dense_4_1_loss: 7.0948 - dense_4_2_loss: 6.9922 - dense_4_3_loss: 6.7060 - dense_4_4_loss: 5.8940 - dense_4_5_loss: 4.7441 - dense_4_6_loss: 3.4884 - dense_4_7_loss: 2.1770 - dense_4_8_loss: 1.1761 - dense_4_9_loss: 0.6574 - dense_4_10_loss: 0.3430 - dense_4_11_loss: 0.2172 - dense_4_accuracy: 4.8828e-04 - dense_4_1_accuracy: 0.0024 - dense_4_2_accuracy: 0.0225 - dense_4_3_accuracy: 0.0830 - dense_4_4_accuracy: 0.2173 - dense_4_5_accuracy: 0.4041 - dense_4_6_accuracy: 0.5908 - dense_4_7_accuracy: 0.7610 - dense_4_8_accuracy: 0.8811 - dense_4_9_accuracy: 0.9434 - dense_4_10_accuracy: 0.9802 - dense_4_11_accuracy: 0.9946\n",
      " \n",
      " Metabatch number --------------------- 21 \n",
      " \n",
      "32/32 [==============================] - 16s 487ms/step - loss: 43.6151 - dense_4_loss: 6.9907 - dense_4_1_loss: 6.9185 - dense_4_2_loss: 6.7536 - dense_4_3_loss: 6.3569 - dense_4_4_loss: 5.5250 - dense_4_5_loss: 4.2788 - dense_4_6_loss: 3.0505 - dense_4_7_loss: 1.8748 - dense_4_8_loss: 0.9892 - dense_4_9_loss: 0.4841 - dense_4_10_loss: 0.2330 - dense_4_11_loss: 0.1601 - dense_4_accuracy: 4.8828e-04 - dense_4_1_accuracy: 7.3242e-04 - dense_4_2_accuracy: 0.0188 - dense_4_3_accuracy: 0.0920 - dense_4_4_accuracy: 0.2437 - dense_4_5_accuracy: 0.4460 - dense_4_6_accuracy: 0.6338 - dense_4_7_accuracy: 0.7869 - dense_4_8_accuracy: 0.8997 - dense_4_9_accuracy: 0.9570 - dense_4_10_accuracy: 0.9888 - dense_4_11_accuracy: 0.9978\n",
      " \n",
      " Metabatch number --------------------- 22 \n",
      " \n",
      "32/32 [==============================] - 16s 487ms/step - loss: 43.4952 - dense_4_loss: 7.1423 - dense_4_1_loss: 6.9358 - dense_4_2_loss: 6.6787 - dense_4_3_loss: 6.3632 - dense_4_4_loss: 5.3940 - dense_4_5_loss: 4.2603 - dense_4_6_loss: 2.9637 - dense_4_7_loss: 1.8553 - dense_4_8_loss: 0.9547 - dense_4_9_loss: 0.4940 - dense_4_10_loss: 0.2758 - dense_4_11_loss: 0.1773 - dense_4_accuracy: 2.4414e-04 - dense_4_1_accuracy: 0.0024 - dense_4_2_accuracy: 0.0347 - dense_4_3_accuracy: 0.1150 - dense_4_4_accuracy: 0.2742 - dense_4_5_accuracy: 0.4683 - dense_4_6_accuracy: 0.6514 - dense_4_7_accuracy: 0.7976 - dense_4_8_accuracy: 0.9053 - dense_4_9_accuracy: 0.9585 - dense_4_10_accuracy: 0.9836 - dense_4_11_accuracy: 0.9956\n",
      " \n",
      " Metabatch number --------------------- 23 \n",
      " \n",
      "32/32 [==============================] - 16s 489ms/step - loss: 41.4518 - dense_4_loss: 6.8445 - dense_4_1_loss: 6.7400 - dense_4_2_loss: 6.5068 - dense_4_3_loss: 6.0694 - dense_4_4_loss: 5.1882 - dense_4_5_loss: 3.9318 - dense_4_6_loss: 2.7682 - dense_4_7_loss: 1.7012 - dense_4_8_loss: 0.9120 - dense_4_9_loss: 0.4413 - dense_4_10_loss: 0.2125 - dense_4_11_loss: 0.1359 - dense_4_accuracy: 4.8828e-04 - dense_4_1_accuracy: 0.0042 - dense_4_2_accuracy: 0.0303 - dense_4_3_accuracy: 0.1135 - dense_4_4_accuracy: 0.2673 - dense_4_5_accuracy: 0.4783 - dense_4_6_accuracy: 0.6582 - dense_4_7_accuracy: 0.8032 - dense_4_8_accuracy: 0.9038 - dense_4_9_accuracy: 0.9580 - dense_4_10_accuracy: 0.9861 - dense_4_11_accuracy: 0.9954\n",
      " \n",
      " Metabatch number --------------------- 24 \n",
      " \n",
      "32/32 [==============================] - 16s 487ms/step - loss: 41.2417 - dense_4_loss: 6.8021 - dense_4_1_loss: 6.6792 - dense_4_2_loss: 6.5216 - dense_4_3_loss: 6.0139 - dense_4_4_loss: 5.1571 - dense_4_5_loss: 4.0482 - dense_4_6_loss: 2.7517 - dense_4_7_loss: 1.6530 - dense_4_8_loss: 0.8843 - dense_4_9_loss: 0.4166 - dense_4_10_loss: 0.1938 - dense_4_11_loss: 0.1202 - dense_4_accuracy: 0.0042 - dense_4_1_accuracy: 0.0027 - dense_4_2_accuracy: 0.0322 - dense_4_3_accuracy: 0.1169 - dense_4_4_accuracy: 0.2803 - dense_4_5_accuracy: 0.4600 - dense_4_6_accuracy: 0.6599 - dense_4_7_accuracy: 0.8103 - dense_4_8_accuracy: 0.9033 - dense_4_9_accuracy: 0.9614 - dense_4_10_accuracy: 0.9861 - dense_4_11_accuracy: 0.9961\n",
      " \n",
      " Metabatch number --------------------- 25 \n",
      " \n",
      "32/32 [==============================] - 16s 487ms/step - loss: 41.5612 - dense_4_loss: 6.8224 - dense_4_1_loss: 6.6601 - dense_4_2_loss: 6.4824 - dense_4_3_loss: 6.0649 - dense_4_4_loss: 5.2312 - dense_4_5_loss: 4.0530 - dense_4_6_loss: 2.8484 - dense_4_7_loss: 1.7789 - dense_4_8_loss: 0.9098 - dense_4_9_loss: 0.4328 - dense_4_10_loss: 0.1845 - dense_4_11_loss: 0.0929 - dense_4_accuracy: 0.0039 - dense_4_1_accuracy: 0.0022 - dense_4_2_accuracy: 0.0334 - dense_4_3_accuracy: 0.1113 - dense_4_4_accuracy: 0.2693 - dense_4_5_accuracy: 0.4685 - dense_4_6_accuracy: 0.6414 - dense_4_7_accuracy: 0.7949 - dense_4_8_accuracy: 0.9009 - dense_4_9_accuracy: 0.9580 - dense_4_10_accuracy: 0.9873 - dense_4_11_accuracy: 0.9983\n",
      " \n",
      " Metabatch number --------------------- 26 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 41.6936 - dense_4_loss: 6.8664 - dense_4_1_loss: 6.7310 - dense_4_2_loss: 6.5166 - dense_4_3_loss: 6.1655 - dense_4_4_loss: 5.2177 - dense_4_5_loss: 4.0403 - dense_4_6_loss: 2.8054 - dense_4_7_loss: 1.7144 - dense_4_8_loss: 0.9115 - dense_4_9_loss: 0.4122 - dense_4_10_loss: 0.1999 - dense_4_11_loss: 0.1127 - dense_4_accuracy: 0.0078 - dense_4_1_accuracy: 0.0042 - dense_4_2_accuracy: 0.0259 - dense_4_3_accuracy: 0.1021 - dense_4_4_accuracy: 0.2639 - dense_4_5_accuracy: 0.4607 - dense_4_6_accuracy: 0.6472 - dense_4_7_accuracy: 0.7998 - dense_4_8_accuracy: 0.9050 - dense_4_9_accuracy: 0.9609 - dense_4_10_accuracy: 0.9858 - dense_4_11_accuracy: 0.9958\n",
      " \n",
      " Metabatch number --------------------- 27 \n",
      " \n",
      "32/32 [==============================] - 16s 490ms/step - loss: 42.7458 - dense_4_loss: 6.8585 - dense_4_1_loss: 6.7426 - dense_4_2_loss: 6.5732 - dense_4_3_loss: 6.1837 - dense_4_4_loss: 5.3363 - dense_4_5_loss: 4.2127 - dense_4_6_loss: 2.9843 - dense_4_7_loss: 1.9146 - dense_4_8_loss: 1.0672 - dense_4_9_loss: 0.5116 - dense_4_10_loss: 0.2336 - dense_4_11_loss: 0.1276 - dense_4_accuracy: 0.0161 - dense_4_1_accuracy: 0.0037 - dense_4_2_accuracy: 0.0288 - dense_4_3_accuracy: 0.1038 - dense_4_4_accuracy: 0.2539 - dense_4_5_accuracy: 0.4426 - dense_4_6_accuracy: 0.6367 - dense_4_7_accuracy: 0.7776 - dense_4_8_accuracy: 0.8828 - dense_4_9_accuracy: 0.9495 - dense_4_10_accuracy: 0.9812 - dense_4_11_accuracy: 0.9934\n",
      " \n",
      " Metabatch number --------------------- 28 \n",
      " \n",
      "32/32 [==============================] - 16s 487ms/step - loss: 40.6297 - dense_4_loss: 6.8234 - dense_4_1_loss: 6.6874 - dense_4_2_loss: 6.4981 - dense_4_3_loss: 6.0308 - dense_4_4_loss: 5.0823 - dense_4_5_loss: 3.9635 - dense_4_6_loss: 2.6470 - dense_4_7_loss: 1.5761 - dense_4_8_loss: 0.7459 - dense_4_9_loss: 0.3382 - dense_4_10_loss: 0.1405 - dense_4_11_loss: 0.0966 - dense_4_accuracy: 0.0327 - dense_4_1_accuracy: 0.0083 - dense_4_2_accuracy: 0.0298 - dense_4_3_accuracy: 0.1145 - dense_4_4_accuracy: 0.2810 - dense_4_5_accuracy: 0.4805 - dense_4_6_accuracy: 0.6790 - dense_4_7_accuracy: 0.8235 - dense_4_8_accuracy: 0.9207 - dense_4_9_accuracy: 0.9690 - dense_4_10_accuracy: 0.9919 - dense_4_11_accuracy: 0.9976\n",
      " \n",
      " Metabatch number --------------------- 29 \n",
      " \n",
      "32/32 [==============================] - 16s 487ms/step - loss: 41.4405 - dense_4_loss: 6.7934 - dense_4_1_loss: 6.7239 - dense_4_2_loss: 6.5236 - dense_4_3_loss: 6.0559 - dense_4_4_loss: 5.1746 - dense_4_5_loss: 4.0328 - dense_4_6_loss: 2.8673 - dense_4_7_loss: 1.7439 - dense_4_8_loss: 0.8875 - dense_4_9_loss: 0.3958 - dense_4_10_loss: 0.1616 - dense_4_11_loss: 0.0803 - dense_4_accuracy: 0.0359 - dense_4_1_accuracy: 0.0024 - dense_4_2_accuracy: 0.0293 - dense_4_3_accuracy: 0.1057 - dense_4_4_accuracy: 0.2671 - dense_4_5_accuracy: 0.4600 - dense_4_6_accuracy: 0.6448 - dense_4_7_accuracy: 0.7959 - dense_4_8_accuracy: 0.9058 - dense_4_9_accuracy: 0.9622 - dense_4_10_accuracy: 0.9873 - dense_4_11_accuracy: 0.9968\n",
      " \n",
      " Metabatch number --------------------- 30 \n",
      " \n",
      "32/32 [==============================] - 16s 488ms/step - loss: 40.7455 - dense_4_loss: 6.7503 - dense_4_1_loss: 6.6494 - dense_4_2_loss: 6.3763 - dense_4_3_loss: 5.9696 - dense_4_4_loss: 5.1158 - dense_4_5_loss: 3.9830 - dense_4_6_loss: 2.7679 - dense_4_7_loss: 1.6602 - dense_4_8_loss: 0.8510 - dense_4_9_loss: 0.3907 - dense_4_10_loss: 0.1536 - dense_4_11_loss: 0.0777 - dense_4_accuracy: 0.0205 - dense_4_1_accuracy: 0.0054 - dense_4_2_accuracy: 0.0337 - dense_4_3_accuracy: 0.1189 - dense_4_4_accuracy: 0.2639 - dense_4_5_accuracy: 0.4678 - dense_4_6_accuracy: 0.6545 - dense_4_7_accuracy: 0.8096 - dense_4_8_accuracy: 0.9089 - dense_4_9_accuracy: 0.9626 - dense_4_10_accuracy: 0.9895 - dense_4_11_accuracy: 0.9976\n",
      " \n",
      " Metabatch number --------------------- 31 \n",
      " \n",
      "32/32 [==============================] - 16s 487ms/step - loss: 39.8633 - dense_4_loss: 6.6019 - dense_4_1_loss: 6.6098 - dense_4_2_loss: 6.3884 - dense_4_3_loss: 5.8215 - dense_4_4_loss: 4.9787 - dense_4_5_loss: 3.8226 - dense_4_6_loss: 2.5388 - dense_4_7_loss: 1.6054 - dense_4_8_loss: 0.8542 - dense_4_9_loss: 0.3933 - dense_4_10_loss: 0.1652 - dense_4_11_loss: 0.0834 - dense_4_accuracy: 0.0212 - dense_4_1_accuracy: 0.0054 - dense_4_2_accuracy: 0.0342 - dense_4_3_accuracy: 0.1189 - dense_4_4_accuracy: 0.2852 - dense_4_5_accuracy: 0.4795 - dense_4_6_accuracy: 0.6755 - dense_4_7_accuracy: 0.8105 - dense_4_8_accuracy: 0.9058 - dense_4_9_accuracy: 0.9607 - dense_4_10_accuracy: 0.9873 - dense_4_11_accuracy: 0.9961\n",
      " \n",
      " Metabatch number --------------------- 32 \n",
      " \n",
      "32/32 [==============================] - 16s 489ms/step - loss: 41.4012 - dense_4_loss: 6.7208 - dense_4_1_loss: 6.6765 - dense_4_2_loss: 6.4788 - dense_4_3_loss: 6.0906 - dense_4_4_loss: 5.1652 - dense_4_5_loss: 4.0642 - dense_4_6_loss: 2.8477 - dense_4_7_loss: 1.7709 - dense_4_8_loss: 0.9232 - dense_4_9_loss: 0.4114 - dense_4_10_loss: 0.1688 - dense_4_11_loss: 0.0832 - dense_4_accuracy: 0.0232 - dense_4_1_accuracy: 0.0027 - dense_4_2_accuracy: 0.0308 - dense_4_3_accuracy: 0.1060 - dense_4_4_accuracy: 0.2627 - dense_4_5_accuracy: 0.4514 - dense_4_6_accuracy: 0.6416 - dense_4_7_accuracy: 0.7903 - dense_4_8_accuracy: 0.9021 - dense_4_9_accuracy: 0.9614 - dense_4_10_accuracy: 0.9873 - dense_4_11_accuracy: 0.9958\n",
      " \n",
      " Metabatch number --------------------- 33 \n",
      " \n",
      "26/26 [==============================] - 29s 479ms/step - loss: 45.1514 - dense_4_loss: 7.2731 - dense_4_1_loss: 7.1372 - dense_4_2_loss: 7.1463 - dense_4_3_loss: 6.6404 - dense_4_4_loss: 5.7162 - dense_4_5_loss: 4.5246 - dense_4_6_loss: 3.0063 - dense_4_7_loss: 1.8743 - dense_4_8_loss: 1.0504 - dense_4_9_loss: 0.4833 - dense_4_10_loss: 0.2151 - dense_4_11_loss: 0.0842 - dense_4_accuracy: 0.0428 - dense_4_1_accuracy: 0.0363 - dense_4_2_accuracy: 0.0295 - dense_4_3_accuracy: 0.1086 - dense_4_4_accuracy: 0.2671 - dense_4_5_accuracy: 0.4564 - dense_4_6_accuracy: 0.6534 - dense_4_7_accuracy: 0.7968 - dense_4_8_accuracy: 0.8989 - dense_4_9_accuracy: 0.9563 - dense_4_10_accuracy: 0.9832 - dense_4_11_accuracy: 0.9963\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for (X_train,Y_train) in BatchGenerator(train_X_indices,one_hot):\n",
    "    m = X_train.shape[0]\n",
    "    s0 = np.zeros((m, n_s))\n",
    "    c0 = np.zeros((m, n_s))\n",
    "    i=i+1\n",
    "    print(f\" \\n Metabatch number --------------------- {i} \\n \")\n",
    "#   print(X_train.shape,Y_train.shape)\n",
    "    kerasmodel.fit([X_train, s0, c0], Y_train, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = kerasmodel.layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(new_weights,open(\"ModelsFeb23/weights_26_1730_6_smallinit\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neweights=kerasmodel.get_layer('embedding').get_weights()[0]\n",
    "#kerasvectors[short_hepmodel.wv.vocab.get(\"witten\").index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomdistance(w1,w2): \n",
    "    num=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    den1=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:])\n",
    "    den2=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    return num/np.sqrt(den1*den2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newdistance(w1,w2): \n",
    "    num=np.dot(new_weights[short_hepmodel.wv.vocab.get(w1).index,:],new_weights[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    den1=np.dot(new_weights[short_hepmodel.wv.vocab.get(w1).index,:],new_weights[short_hepmodel.wv.vocab.get(w1).index,:])\n",
    "    den2=np.dot(new_weights[short_hepmodel.wv.vocab.get(w2).index,:],new_weights[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    return num/np.sqrt(den1*den2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def olddistance(w1,w2): \n",
    "    num=np.dot(short_hepmodel.wv[w1].T,short_hepmodel.wv[w2])\n",
    "    den1=np.dot(short_hepmodel.wv[w1].T,short_hepmodel.wv[w1])\n",
    "    den2=np.dot(short_hepmodel.wv[w2].T,short_hepmodel.wv[w2])\n",
    "    return num/np.sqrt(den1*den2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bigrav', 0.5847691297531128),\n",
       " ('mwand', 0.5196281671524048),\n",
       " ('gravit', 0.49977532029151917),\n",
       " ('tmg', 0.4800078868865967),\n",
       " ('aether', 0.46520933508872986),\n",
       " ('caliz', 0.4608544111251831),\n",
       " ('horndeski', 0.46077531576156616),\n",
       " ('maxwel', 0.45286959409713745),\n",
       " ('galileon', 0.4516640305519104),\n",
       " ('vcc', 0.4507347047328949),\n",
       " ('evera', 0.4476248621940613),\n",
       " ('rel', 0.4465886652469635),\n",
       " ('electrodynam', 0.44282782077789307),\n",
       " ('cosmolog', 0.4363742768764496),\n",
       " ('palatini', 0.43545424938201904)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_hepmodel.wv.most_similar(positive=[\"graviti\"],topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9406544"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdistance(\"graviti\",\"newtonian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(w1,w2):\n",
    "    print(f\"Random distance : {randomdistance(w1,w2)} \")    \n",
    "    print(f\"Word2Vec distance : {olddistance(w1,w2)} \")\n",
    "    print(f\"Our LSTM distance : {newdistance(w1,w2)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.07818636634452886 \n",
      "Word2Vec distance : 0.48400288820266724 \n",
      "Our LSTM distance : 0.9903555512428284 \n"
     ]
    }
   ],
   "source": [
    "distance(\"gener\",\"relat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.08210770744612497 \n",
      "Word2Vec distance : 0.024747705087065697 \n",
      "Our LSTM distance : 0.9620609879493713 \n"
     ]
    }
   ],
   "source": [
    "distance(\"witten\",\"brane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only testing below. No new code. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.1735790213126782 \n",
      "Word2Vec distance : 0.5095011591911316 \n",
      "Our LSTM distance : 0.4693949818611145 \n"
     ]
    }
   ],
   "source": [
    "distance(\"hawk\",\"blackhol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.007357943536875019 \n",
      "Word2Vec distance : 0.10821077227592468 \n",
      "Our LSTM distance : 0.13437694311141968 \n"
     ]
    }
   ],
   "source": [
    "distance(\"hawk\",\"information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.004614738589563649 \n",
      "Word2Vec distance : 0.36223238706588745 \n",
      "Our LSTM distance : 0.1721714437007904 \n"
     ]
    }
   ],
   "source": [
    "distance(\"information\",\"paradox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.056995172560698575 \n",
      "Word2Vec distance : 0.3886634409427643 \n",
      "Our LSTM distance : 0.9219940304756165 \n"
     ]
    }
   ],
   "source": [
    "distance(\"witten\",\"seiberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.05183285556678813 \n",
      "Word2Vec distance : 0.3954869210720062 \n",
      "Our LSTM distance : 0.9247261881828308 \n"
     ]
    }
   ],
   "source": [
    "distance(\"holographi\",\"maldacena\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.21007742574811286 \n",
      "Word2Vec distance : 0.19099871814250946 \n",
      "Our LSTM distance : 0.9247686266899109 \n"
     ]
    }
   ],
   "source": [
    "distance(\"witten\",\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.3543670814664644 \n",
      "Word2Vec distance : 0.3977150022983551 \n",
      "Our LSTM distance : 0.7770392298698425 \n"
     ]
    }
   ],
   "source": [
    "distance(\"diagram\",\"feynman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.08210770744612497 \n",
      "Word2Vec distance : 0.024747705087065697 \n",
      "Our LSTM distance : 0.9620609879493713 \n"
     ]
    }
   ],
   "source": [
    "distance(\"brane\",\"witten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.3960001742696835 \n",
      "Word2Vec distance : -0.05112694203853607 \n",
      "Our LSTM distance : -0.018089938908815384 \n"
     ]
    }
   ],
   "source": [
    "distance(\"wilson\",\"rg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.12097695160003447 \n",
      "Word2Vec distance : 0.2348415106534958 \n",
      "Our LSTM distance : 0.9697962999343872 \n"
     ]
    }
   ],
   "source": [
    "distance(\"renormaliz\",\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.13312005101571397 \n",
      "Word2Vec distance : 0.30107200145721436 \n",
      "Our LSTM distance : 0.3115948438644409 \n"
     ]
    }
   ],
   "source": [
    "distance(\"graviti\",\"nonrenormaliz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.08026520653807753 \n",
      "Word2Vec distance : 0.2914222478866577 \n",
      "Our LSTM distance : 0.8830674886703491 \n"
     ]
    }
   ],
   "source": [
    "distance(\"pauli\",\"heisenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.007398052373150316 \n",
      "Word2Vec distance : -0.36764681339263916 \n",
      "Our LSTM distance : 0.0006296865758486092 \n"
     ]
    }
   ],
   "source": [
    "distance(\"isi\",\"fix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.25965694463058264 \n",
      "Word2Vec distance : 0.17695051431655884 \n",
      "Our LSTM distance : 0.9747143387794495 \n"
     ]
    }
   ],
   "source": [
    "distance(\"einstein\",\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.15399388292422278 \n",
      "Word2Vec distance : -0.020672762766480446 \n",
      "Our LSTM distance : 0.9815539121627808 \n"
     ]
    }
   ],
   "source": [
    "distance(\"graviti\",\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.16424334228916998 \n",
      "Word2Vec distance : 0.05436236783862114 \n",
      "Our LSTM distance : 0.9826259016990662 \n"
     ]
    }
   ],
   "source": [
    "distance(\"string\",\"graviti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.07576074264608211 \n",
      "Word2Vec distance : -0.23231807351112366 \n",
      "Our LSTM distance : -0.06802619248628616 \n"
     ]
    }
   ],
   "source": [
    "distance(\"isi\",\"conform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.12989734355216886 \n",
      "Word2Vec distance : -0.09121870994567871 \n",
      "Our LSTM distance : -0.04096447303891182 \n"
     ]
    }
   ],
   "source": [
    "distance(\"isi\",\"cft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.012516211692843206 \n",
      "Word2Vec distance : 0.36304622888565063 \n",
      "Our LSTM distance : 0.4926113784313202 \n"
     ]
    }
   ],
   "source": [
    "distance(\"mellin\",\"bootstrap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.14368135532280366 \n",
      "Word2Vec distance : 0.15752534568309784 \n",
      "Our LSTM distance : 0.9911961555480957 \n"
     ]
    }
   ],
   "source": [
    "distance(\"string\",\"theori\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.05739704625072687 \n",
      "Word2Vec distance : 0.17478995025157928 \n",
      "Our LSTM distance : 0.9895001649856567 \n"
     ]
    }
   ],
   "source": [
    "distance(\"quantum\",\"theori\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.21950370784678408 \n",
      "Word2Vec distance : 0.6166797280311584 \n",
      "Our LSTM distance : 0.9212174415588379 \n"
     ]
    }
   ],
   "source": [
    "distance(\"quark\",\"color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.03902662731800803 \n",
      "Word2Vec distance : 0.25014472007751465 \n",
      "Our LSTM distance : 0.9779474139213562 \n"
     ]
    }
   ],
   "source": [
    "distance(\"gaug\",\"confin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.1533657600785285 \n",
      "Word2Vec distance : 0.20845641195774078 \n",
      "Our LSTM distance : 0.8777956366539001 \n"
     ]
    }
   ],
   "source": [
    "distance(\"wilson\",\"confin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.020526790108468727 \n",
      "Word2Vec distance : 0.563359260559082 \n",
      "Our LSTM distance : 0.9275000095367432 \n"
     ]
    }
   ],
   "source": [
    "distance(\"quark\",\"confin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(new_weights,open(\"ModelsFeb23/weights_26_1600_1_smallinit\",'wb'))\n",
    "#pickle.dump(short_hepmodel,open(\"ModelsFeb23/short_hepmodel_26_1600_1_smallinit\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1=pickle.load(open(\"ModelsFeb23/weights_26_1613_1_smallinit\",'rb'))\n",
    "weights3=pickle.load(open(\"ModelsFeb23/weights_26_1630_3_smallinit\",'rb'))\n",
    "weights6=pickle.load(open(\"ModelsFeb23/weights_26_1630_6_smallinit\",'rb'))\n",
    "#tempmodel=pickle.load(open(\"ModelsFeb23/short_hepmodel_24_1957\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tempweights[tempmodel.wv.vocab.get(\"witten\").index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickledistance(w1,w2,new_weights):\n",
    "    num=np.dot(new_weights[short_hepmodel.wv.vocab.get(w1).index,:],new_weights[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    den1=np.dot(new_weights[short_hepmodel.wv.vocab.get(w1).index,:],new_weights[short_hepmodel.wv.vocab.get(w1).index,:])\n",
    "    den2=np.dot(new_weights[short_hepmodel.wv.vocab.get(w2).index,:],new_weights[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    newdist= num/np.sqrt(den1*den2)\n",
    "    num=np.dot(short_hepmodel.wv[w1].T,short_hepmodel.wv[w2])\n",
    "    den1=np.dot(short_hepmodel.wv[w1].T,short_hepmodel.wv[w1])\n",
    "    den2=np.dot(short_hepmodel.wv[w2].T,short_hepmodel.wv[w2])\n",
    "    olddist= num/np.sqrt(den1*den2)\n",
    "    num=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    den1=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:])\n",
    "    den2=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    rand= num/np.sqrt(den1*den2)\n",
    "    print(f\"Random distance : {rand} \")    \n",
    "    print(f\"Word2Vec distance : {olddist} \")\n",
    "    print(f\"Our LSTM distance : {newdist} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.0930195273435413 \n",
      "Word2Vec distance : 0.040753476321697235 \n",
      "Our LSTM distance : 0.371532142162323 \n"
     ]
    }
   ],
   "source": [
    "pickledistance(\"witten\",\"brane\",weights6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.1161497091576768 \n",
      "Word2Vec distance : -0.022293496876955032 \n",
      "Our LSTM distance : 0.7281190752983093 \n"
     ]
    }
   ],
   "source": [
    "pickledistance(\"wilson\",\"rg\",weights6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.10506595089579158 \n",
      "Word2Vec distance : 0.1636536419391632 \n",
      "Our LSTM distance : -0.214313343167305 \n"
     ]
    }
   ],
   "source": [
    "pickledistance(\"string\",\"theori\",weights6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_latest_p37]",
   "language": "python",
   "name": "conda-env-tensorflow2_latest_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
