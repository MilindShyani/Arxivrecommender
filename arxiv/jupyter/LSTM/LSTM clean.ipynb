{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM and a proxy task of paper title predicition to get better Word vectors #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json, gensim, sklearn, pickle, sys, re, os\n",
    "import IPython.display as ipd\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_short, strip_multiple_whitespaces, stem_text\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from IPython.display import Audio\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "sound_file = './Music/invalid_keypress.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapd=pd.read_json(\"/home/ubuntu/project/arxiv-metadata-oai-snapshot.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapd_hep_th = datapd.loc[datapd['categories'].str.contains('hep-th')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_abstracts = datapd_hep_th[ list(datapd_hep_th.iloc[:,0:1]) + ['abstract'] + ['title']+['authors']]\n",
    "hep_abstracts_limit=hep_abstracts[hep_abstracts.title.str.count(' ') < 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_corpus_limit=hep_abstracts_limit['abstract'].values \n",
    "hep_corpus_limit_eof=hep_corpus_limit+\" EOF\"\n",
    "short_hep_abs=hep_corpus_limit_eof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_titles=hep_abstracts_limit['title'].values\n",
    "hep_titles_eof=hep_titles + \" EOF\"\n",
    "short_hep_tit=hep_titles_eof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_hep=preprocess_documents(hep_corpus_limit)\n",
    "#pickle.dump(processed_hep,open(\"processed_hep\",\"wb\"))\n",
    "processed_hep=pickle.load(open(\"/home/ubuntu/project/LSTM/Mar4bigtrainingWord/processed_hep\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hep_dictionary_lim = gensim.corpora.Dictionary(processed_hep)\n",
    "#hep_bow_corpus_lim = [hep_dictionary_lim.doc2bow(text) for text in processed_hep]\n",
    "#pickle.dump(hep_dictionary_lim,open(\"hep_dictionary_lim\",\"wb\"))\n",
    "#pickle.dump(hep_bow_corpus_lim,open(\"hep_bow_corpus_lim\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_dictionary_lim=pickle.load(open(\"/home/ubuntu/project/LSTM/Mar4bigtrainingWord/hep_dictionary_lim\",\"rb\"))\n",
    "hep_bow_corpus_lim=pickle.load(open(\"/home/ubuntu/project/LSTM/Mar4bigtrainingWord/hep_bow_corpus_lim\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_tfidf = gensim.models.TfidfModel(hep_bow_corpus_lim, smartirs='npu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = gensim.similarities.MatrixSimilarity(hep_tfidf[hep_bow_corpus_lim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By doing the above, we have obtained the 130K papers that belong in high energy theory physics (hep-th) ###\n",
    "\n",
    "### We restrict to paper with titles shorter than 16 words. We also add an \" EOF\" to indicate the end of the abstract/title ###\n",
    "\n",
    "## We will now process the documents using gensim. This removes stop words, punctuations, capitalization and stems and lemmatizes the tezt ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokenized_nostop=preprocess_documents(short_hep_abs)\n",
    "doctitle_tokenized_nostop=preprocess_documents(short_hep_tit)\n",
    "#pickle.dump(doc_tokenized_nostop,open(\"Modelsfeb18/doc_tokenized_nostop\", 'wb'))\n",
    "#pickle.dump(doc_tokenized_nostop,open(\"Modelsfeb18/doctitle_tokenized_nostop\", 'wb'))\n",
    "#doc_tokenized_nostop=pickle.load(open(\"/home/ubuntu/Modelsfeb18/doc_tokenized_nostop\", 'rb'))\n",
    "#doctitle_tokenized_nostop=pickle.load(open(\"/home/ubuntu/Modelsfeb18/doctitle_tokenized_nostop\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leabs = [len(x) for x in doc_tokenized_nostop]\n",
    "letit = [len(x) for x in doctitle_tokenized_nostop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for x in range(len(leabs)):\n",
    "    if leabs[x]<=155:\n",
    "        count=count+1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for x in range(len(letit)):\n",
    "    if letit[x]<=12:\n",
    "        count=count+1\n",
    "        #print(x)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above tells us that almost 95% of the total number of hep-th papers (140,500) have abstracts shorter than 155 words and titles shorter than 12 words. We will use this fact below when constructing our LSTM network ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(letit),max(leabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_tokenized_nostop[225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctitle_tokenized_nostop[225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size=52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we train Word2Vec. This will learn word embeddings ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below line trains Word2Vec on all the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#short_hepmodel=Word2Vec(sentences=doc_tokenized_nostop,size=vec_size, window=5, min_count=1, workers=4, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_hepmodel=pickle.load(open(\"/home/ubuntu/project/LSTM/Modelsfeb18/hepmodel_mar_1_1538\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab=len(short_hepmodel.wv.vocab)\n",
    "print(totalvocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below further trains the model on all the paper titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#short_hepmodel.build_vocab(doctitle_tokenized_nostop, update=True)\n",
    "#short_hepmodel.train(doctitle_tokenized_nostop, total_examples=short_hepmodel.corpus_count, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab=len(short_hepmodel.wv.vocab)\n",
    "print(totalvocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(short_hepmodel,open(\"Modelsfeb18/hepmodel_mar_1_1538\", 'wb'))\n",
    "#short_hepmodel=pickle.load(open('ModelsFeb23/hepmodel_feb_26_1558','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising using t-SNE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,vec_size), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    close_words = model.wv.similar_by_word(word,topn=15)\n",
    "    \n",
    "\n",
    "    arr = np.append(arr, np.array([model.wv[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 5), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()-15, x_coords.max()+15)\n",
    "    plt.ylim(y_coords.min()-15, y_coords.max()+15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(short_hepmodel, 'newton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_hepmodel.wv.most_similar(positive=[\"newton\"],topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will train an LSTM network with attention to get better word embeddings #\n",
    "\n",
    "## The aim of the task is to predict paper titles from the abstract. This is an impossibly hard task, but hopefully we will learn good word emebeddings along the way ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras  \n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting max length of the abstract and title to be 155 and 12 respectively. As shown above this is a reasonable thing to do ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx=155\n",
    "Ty=12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below function takes in sentences, and out puts a list of numbers. Each number correspond to a word. Thus the range of the numbers is from 0 to length of the vocab.  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, hepmodel,max_len):\n",
    "    m = X.shape[0] \n",
    "    X_indices = np.zeros((m,max_len),dtype='uint16')+hepmodel.wv.vocab.get('eof').index\n",
    "    for i in range(m):                          \n",
    "        sentence_words = X[i]\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            X_indices[i, j] = hepmodel.wv.vocab.get(w).index\n",
    "            j = j+1\n",
    "            if j>=max_len:\n",
    "                break\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_abs=np.asarray(doc_tokenized_nostop)\n",
    "array_tit=np.asarray(doctitle_tokenized_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X_indices=sentences_to_indices(array_abs,short_hepmodel,Tx)\n",
    "train_Y_indices=sentences_to_indices(array_tit,short_hepmodel,Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_indices[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below function creates a small dense network for the attention layer ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import softmax\n",
    "def softmax_axis1(x):\n",
    "    return softmax(x,axis=1)\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(Tx)\n",
    "densor2 = Dense(Tx)\n",
    "densor3 = Dense(10)\n",
    "densor4 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax_axis1, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    s_prev = repeator(s_prev)\n",
    "    concat = concatenator([a,s_prev])\n",
    "\n",
    "    e = densor1(concat)\n",
    "    e = tf.keras.layers.LeakyReLU(alpha=0.3)(e)\n",
    "\n",
    "\n",
    "    e = densor3(e)\n",
    "    e = tf.keras.layers.LeakyReLU(alpha=0.3)(e)\n",
    "\n",
    "    energies = densor4(e)\n",
    "    alphas = activator(energies)\n",
    "    context = dotor([alphas,a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = [['witten' 'unif' 'string' 'theori']\n",
      " ['dirac' 'discover' 'field' 'theori']]\n",
      "X1_indices =\n",
      " [[  426   969    10     0     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1]\n",
      " [  236 13827     2     0     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1     1\n",
      "      1     1     1     1     1     1     1     1     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([['witten', 'unif', 'string', 'theori' ],['dirac', 'discover', 'field' , 'theori']])\n",
    "X1_indices = sentences_to_indices(X1, short_hepmodel,max_len = Tx)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's set up the LSTM network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True,return_sequences = True ) \n",
    "output_layer = Dense(totalvocab, activation=softmax_axis1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will now set up the Keras embedding layer. We initialize the Keras embedding matrix using our pretrained Word2Vec vectors. ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_vectors=short_hepmodel.wv.vectors\n",
    "appker=np.zeros((1,vec_size))\n",
    "kerasvectors=np.vstack((small_vectors,appker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the embedding layer and set *trainable=True*. We will read the weights from this layer once the network has trained. ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(hepmodel):\n",
    "    vocab_len = totalvocab + 1                  \n",
    "    emb_dim = hepmodel.wv[\"newton\"].shape[0]      \n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    embedding_layer = Embedding(vocab_len,emb_dim,trainable=True)\n",
    "    embedding_layer.build((None,)) \n",
    "    embedding_layer.set_weights([kerasvectors])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1628][12] = -0.15372208\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(short_hepmodel)\n",
    "print(\"weights[0][1628][12] =\", embedding_layer.get_weights()[0][1628][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  83,  491,   63, ...,    1,    1,    1],\n",
       "       [4041,    4, 8213, ...,    1,    1,    1],\n",
       "       [   6,    2,    0, ...,    1,    1,    1],\n",
       "       ...,\n",
       "       [3399,  955,  541, ...,    1,    1,    1],\n",
       "       [ 158, 3117,  541, ...,   91,    1,    1],\n",
       "       [  10,    3, 2707, ...,    1,    1,    1]], dtype=uint16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model. 1 bi-directional LSTM --> Attention layer ---> LSTM ----> Softmax. The output is (Ty) number of column vectors of dimension (totalvocab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerasmagic(input_shape, Tx, Ty, n_a, n_s, totalvocab, hepmodel): \n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "    contextual = []\n",
    "    hidden_state = []\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer \n",
    "    embedding_layer = pretrained_embedding_layer(hepmodel)\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    \n",
    "    b = Bidirectional(LSTM(units=n_a, return_sequences=True))(embeddings)\n",
    "    \n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b)\n",
    "    \n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b)\n",
    "    \n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b)\n",
    "    \n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b)\n",
    "\n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b)\n",
    "    \n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b)\n",
    "    \n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b)\n",
    "    \n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b)\n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b)\n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b)\n",
    "    #b = Bidirectional(LSTM(units=n_a, return_sequences=True))(b) \n",
    "    \n",
    "        \n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Perform one step of the attention mechanism #\n",
    "        context = one_step_attention(b,s)\n",
    "        contextual.append(context)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        _, s , c = post_activation_LSTM_cell(context, initial_state=[s,c])\n",
    "        hidden_state.append(s)\n",
    "        out = output_layer(s)\n",
    "        outputs.append(out)\n",
    "        \n",
    "        \n",
    "    array_out=tf.convert_to_tensor(outputs)\n",
    "    array_outputs=tf.transpose(array_out,[1,0,2])\n",
    "\n",
    "\n",
    "    model1 = Model([sentence_indices,s0,c0],array_outputs)\n",
    "    model2 = Model([sentence_indices,s0,c0],[outputs,hidden_state,contextual])\n",
    "    \n",
    "    return model1, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasmodel=kerasmagic(Tx,Tx,Ty,n_a,n_s,totalvocab,short_hepmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 155)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 155, 52)      1864408     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 155, 128)     59904       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 155, 64)      0           s0[0][0]                         \n",
      "                                                                 lstm[24][1]                      \n",
      "                                                                 lstm[25][1]                      \n",
      "                                                                 lstm[26][1]                      \n",
      "                                                                 lstm[27][1]                      \n",
      "                                                                 lstm[28][1]                      \n",
      "                                                                 lstm[29][1]                      \n",
      "                                                                 lstm[30][1]                      \n",
      "                                                                 lstm[31][1]                      \n",
      "                                                                 lstm[32][1]                      \n",
      "                                                                 lstm[33][1]                      \n",
      "                                                                 lstm[34][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 155, 192)     0           bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[24][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[25][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[26][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[27][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[28][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[29][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[30][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[31][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[32][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[33][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[34][0]             \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 repeat_vector[35][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 155, 155)     29915       concatenate[24][0]               \n",
      "                                                                 concatenate[25][0]               \n",
      "                                                                 concatenate[26][0]               \n",
      "                                                                 concatenate[27][0]               \n",
      "                                                                 concatenate[28][0]               \n",
      "                                                                 concatenate[29][0]               \n",
      "                                                                 concatenate[30][0]               \n",
      "                                                                 concatenate[31][0]               \n",
      "                                                                 concatenate[32][0]               \n",
      "                                                                 concatenate[33][0]               \n",
      "                                                                 concatenate[34][0]               \n",
      "                                                                 concatenate[35][0]               \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, 155, 155)     0           dense[24][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 155, 10)      1560        leaky_re_lu_48[0][0]             \n",
      "                                                                 leaky_re_lu_50[0][0]             \n",
      "                                                                 leaky_re_lu_52[0][0]             \n",
      "                                                                 leaky_re_lu_54[0][0]             \n",
      "                                                                 leaky_re_lu_56[0][0]             \n",
      "                                                                 leaky_re_lu_58[0][0]             \n",
      "                                                                 leaky_re_lu_60[0][0]             \n",
      "                                                                 leaky_re_lu_62[0][0]             \n",
      "                                                                 leaky_re_lu_64[0][0]             \n",
      "                                                                 leaky_re_lu_66[0][0]             \n",
      "                                                                 leaky_re_lu_68[0][0]             \n",
      "                                                                 leaky_re_lu_70[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)      (None, 155, 10)      0           dense_2[24][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 155, 1)       11          leaky_re_lu_49[0][0]             \n",
      "                                                                 leaky_re_lu_51[0][0]             \n",
      "                                                                 leaky_re_lu_53[0][0]             \n",
      "                                                                 leaky_re_lu_55[0][0]             \n",
      "                                                                 leaky_re_lu_57[0][0]             \n",
      "                                                                 leaky_re_lu_59[0][0]             \n",
      "                                                                 leaky_re_lu_61[0][0]             \n",
      "                                                                 leaky_re_lu_63[0][0]             \n",
      "                                                                 leaky_re_lu_65[0][0]             \n",
      "                                                                 leaky_re_lu_67[0][0]             \n",
      "                                                                 leaky_re_lu_69[0][0]             \n",
      "                                                                 leaky_re_lu_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 155, 1)       0           dense_3[24][0]                   \n",
      "                                                                 dense_3[25][0]                   \n",
      "                                                                 dense_3[26][0]                   \n",
      "                                                                 dense_3[27][0]                   \n",
      "                                                                 dense_3[28][0]                   \n",
      "                                                                 dense_3[29][0]                   \n",
      "                                                                 dense_3[30][0]                   \n",
      "                                                                 dense_3[31][0]                   \n",
      "                                                                 dense_3[32][0]                   \n",
      "                                                                 dense_3[33][0]                   \n",
      "                                                                 dense_3[34][0]                   \n",
      "                                                                 dense_3[35][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 128)       0           attention_weights[24][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[25][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[26][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[27][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[28][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[29][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[30][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[31][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[32][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[33][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[34][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "                                                                 attention_weights[35][0]         \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 1, 64), (Non 49408       dot[24][0]                       \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot[25][0]                       \n",
      "                                                                 lstm[24][1]                      \n",
      "                                                                 lstm[24][2]                      \n",
      "                                                                 dot[26][0]                       \n",
      "                                                                 lstm[25][1]                      \n",
      "                                                                 lstm[25][2]                      \n",
      "                                                                 dot[27][0]                       \n",
      "                                                                 lstm[26][1]                      \n",
      "                                                                 lstm[26][2]                      \n",
      "                                                                 dot[28][0]                       \n",
      "                                                                 lstm[27][1]                      \n",
      "                                                                 lstm[27][2]                      \n",
      "                                                                 dot[29][0]                       \n",
      "                                                                 lstm[28][1]                      \n",
      "                                                                 lstm[28][2]                      \n",
      "                                                                 dot[30][0]                       \n",
      "                                                                 lstm[29][1]                      \n",
      "                                                                 lstm[29][2]                      \n",
      "                                                                 dot[31][0]                       \n",
      "                                                                 lstm[30][1]                      \n",
      "                                                                 lstm[30][2]                      \n",
      "                                                                 dot[32][0]                       \n",
      "                                                                 lstm[31][1]                      \n",
      "                                                                 lstm[31][2]                      \n",
      "                                                                 dot[33][0]                       \n",
      "                                                                 lstm[32][1]                      \n",
      "                                                                 lstm[32][2]                      \n",
      "                                                                 dot[34][0]                       \n",
      "                                                                 lstm[33][1]                      \n",
      "                                                                 lstm[33][2]                      \n",
      "                                                                 dot[35][0]                       \n",
      "                                                                 lstm[34][1]                      \n",
      "                                                                 lstm[34][2]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)      (None, 155, 155)     0           dense[25][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, 155, 10)      0           dense_2[25][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, 155, 155)     0           dense[26][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)      (None, 155, 10)      0           dense_2[26][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, 155, 155)     0           dense[27][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, 155, 10)      0           dense_2[27][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, 155, 155)     0           dense[28][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, 155, 10)      0           dense_2[28][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)      (None, 155, 155)     0           dense[29][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, 155, 10)      0           dense_2[29][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)      (None, 155, 155)     0           dense[30][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)      (None, 155, 10)      0           dense_2[30][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)      (None, 155, 155)     0           dense[31][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)      (None, 155, 10)      0           dense_2[31][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)      (None, 155, 155)     0           dense[32][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)      (None, 155, 10)      0           dense_2[32][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)      (None, 155, 155)     0           dense[33][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)      (None, 155, 10)      0           dense_2[33][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)      (None, 155, 155)     0           dense[34][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)      (None, 155, 10)      0           dense_2[34][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)      (None, 155, 155)     0           dense[35][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)      (None, 155, 10)      0           dense_2[35][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 35853)        2330445     lstm[24][1]                      \n",
      "                                                                 lstm[25][1]                      \n",
      "                                                                 lstm[26][1]                      \n",
      "                                                                 lstm[27][1]                      \n",
      "                                                                 lstm[28][1]                      \n",
      "                                                                 lstm[29][1]                      \n",
      "                                                                 lstm[30][1]                      \n",
      "                                                                 lstm[31][1]                      \n",
      "                                                                 lstm[32][1]                      \n",
      "                                                                 lstm[33][1]                      \n",
      "                                                                 lstm[34][1]                      \n",
      "                                                                 lstm[35][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor_2 (TFOpLam (12, None, 35853)    0           dense_4[24][0]                   \n",
      "                                                                 dense_4[25][0]                   \n",
      "                                                                 dense_4[26][0]                   \n",
      "                                                                 dense_4[27][0]                   \n",
      "                                                                 dense_4[28][0]                   \n",
      "                                                                 dense_4[29][0]                   \n",
      "                                                                 dense_4[30][0]                   \n",
      "                                                                 dense_4[31][0]                   \n",
      "                                                                 dense_4[32][0]                   \n",
      "                                                                 dense_4[33][0]                   \n",
      "                                                                 dense_4[34][0]                   \n",
      "                                                                 dense_4[35][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_2 (TFOpL (None, 12, 35853)    0           tf.convert_to_tensor_2[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 4,335,651\n",
      "Trainable params: 4,335,651\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kerasmodel[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.0)\n",
    "kerasmodel[0].compile(optimizer=opt, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "kerasmodel[1].compile(optimizer=opt, loss=['categorical_crossentropy','categorical_crossentropy'], metrics=['accuracy'],loss_weights=[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note we compile two Keras models. But only train the first one. The second model is to just used to obtain the contextual and hidden states #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theori'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_hepmodel.wv.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41631760\n",
      "3223104\n"
     ]
    }
   ],
   "source": [
    "print(train_X_indices.size*train_X_indices.itemsize)\n",
    "print(train_Y_indices.size*train_Y_indices.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### echo 1 | sudo tee /proc/sys/vm/overcommit_memory ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = totalvocab\n",
    "text_length = len(train_Y_indices[0])\n",
    "one_hot = np.zeros([train_X_indices.shape[0],vocab_size,text_length],dtype='uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_X_indices.shape[0]):\n",
    "    for j in range(Ty):\n",
    "        one_hot[i,train_Y_indices[i,j],j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "listhot = list(one_hot.T.swapaxes(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134296, 35853, 12)\n",
      "(134296, 155)\n",
      "(134296, 35853)\n"
     ]
    }
   ],
   "source": [
    "print(one_hot.shape)\n",
    "print(train_X_indices.shape)\n",
    "print(listhot[7].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_shuffled, Y_shuffled = shuffle(train_X_indices,train_Y_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_shuffled = np.zeros([X_shuffled.shape[0],vocab_size,text_length],dtype='uint16')\n",
    "for i in range(X_shuffled.shape[0]):\n",
    "    for j in range(Ty):\n",
    "        one_hot_shuffled[i,Y_shuffled[i,j],j]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now use a batch generator, since the system runs out of memory otherwise #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_size=16384\n",
    "def BatchGenerator(X_shuffled,one_hot_shuffled):\n",
    "    for i in range(1,9):\n",
    "        X_train = X_shuffled[exp_size*(i-1):exp_size*i,:]\n",
    "        one_hot_sliced_shuffled = one_hot_shuffled[exp_size*(i-1):exp_size*i,:,:]\n",
    "        Y_train = list(one_hot_sliced_shuffled.T.swapaxes(1,2))\n",
    "        yield (X_train, Y_train)\n",
    "    X_train = X_shuffled[exp_size*i:-1,:]\n",
    "    one_hot_sliced_shuffled = one_hot_shuffled[exp_size*i:-1,:,:]\n",
    "    Y_train = list(one_hot_sliced_shuffled.T.swapaxes(1,2))\n",
    "    yield (X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Metabatch number --------------------- 1 \n",
      " \n",
      "128/128 [==============================] - 49s 382ms/step - loss: 4.6583 - accuracy: 0.1651\n",
      " \n",
      " Metabatch number --------------------- 2 \n",
      " \n",
      "128/128 [==============================] - 48s 374ms/step - loss: 4.6250 - accuracy: 0.1673\n",
      " \n",
      " Metabatch number --------------------- 3 \n",
      " \n",
      "128/128 [==============================] - 48s 373ms/step - loss: 4.5706 - accuracy: 0.1691\n",
      " \n",
      " Metabatch number --------------------- 4 \n",
      " \n",
      "128/128 [==============================] - 51s 398ms/step - loss: 4.5204 - accuracy: 0.1706\n",
      " \n",
      " Metabatch number --------------------- 5 \n",
      " \n",
      "128/128 [==============================] - 48s 374ms/step - loss: 4.5245 - accuracy: 0.1715\n",
      " \n",
      " Metabatch number --------------------- 6 \n",
      " \n",
      "128/128 [==============================] - 48s 374ms/step - loss: 4.4926 - accuracy: 0.1737\n",
      " \n",
      " Metabatch number --------------------- 7 \n",
      " \n",
      "128/128 [==============================] - 48s 376ms/step - loss: 4.4626 - accuracy: 0.1757\n",
      " \n",
      " Metabatch number --------------------- 8 \n",
      " \n",
      "128/128 [==============================] - 48s 373ms/step - loss: 4.4421 - accuracy: 0.1776\n",
      " \n",
      " Metabatch number --------------------- 9 \n",
      " \n",
      "26/26 [==============================] - 10s 366ms/step - loss: 4.4203 - accuracy: 0.1786\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for (X_train,Y_train) in BatchGenerator(X_shuffled,one_hot_shuffled):\n",
    "    m = X_train.shape[0]\n",
    "    s0 = np.zeros((m, n_s))\n",
    "    c0 = np.zeros((m, n_s))\n",
    "    Yarraytemp=np.array(Y_train)\n",
    "    Yarray=np.swapaxes(Yarraytemp,0,1)\n",
    "    i=i+1\n",
    "    print(f\" \\n Metabatch number --------------------- {i} \\n \")\n",
    "    kerasmodel[0].fit([X_train, s0, c0], Yarray, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After having trained the model. We have two choices available to create abstract vectors. The first is to use the finetuned word vectors and the second is to use the hidden states ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfortunately the Hidden states do not lead to meaninful predictions, and so we now focus on the word vectors. Neverthless the code for using Hidden states can be found at the end of this notebook ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = kerasmodel[0].layers[1].get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the weights #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(new_weights,open(\"Weights_v3\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we define functions to check the ditances between word vectors. Old vs new distance corresponds pre and post fine tuning. While random distance corresponds to a random initialization. ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomdistance(w1,w2): \n",
    "    num=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    den1=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:])\n",
    "    den2=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    return num/np.sqrt(den1*den2)\n",
    "def newdistance(w1,w2): \n",
    "    num=np.dot(new_weights[short_hepmodel.wv.vocab.get(w1).index,:],new_weights[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    den1=np.dot(new_weights[short_hepmodel.wv.vocab.get(w1).index,:],new_weights[short_hepmodel.wv.vocab.get(w1).index,:])\n",
    "    den2=np.dot(new_weights[short_hepmodel.wv.vocab.get(w2).index,:],new_weights[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    return num/np.sqrt(den1*den2)\n",
    "def olddistance(w1,w2): \n",
    "    num=np.dot(short_hepmodel.wv[w1].T,short_hepmodel.wv[w2])\n",
    "    den1=np.dot(short_hepmodel.wv[w1].T,short_hepmodel.wv[w1])\n",
    "    den2=np.dot(short_hepmodel.wv[w2].T,short_hepmodel.wv[w2])\n",
    "    return num/np.sqrt(den1*den2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vafa', 0.8461833000183105),\n",
       " ('seiberg', 0.807892918586731),\n",
       " ('gromov', 0.7971237897872925),\n",
       " ('dijkgraaf', 0.7869737148284912),\n",
       " ('gopakumar', 0.7534626722335815),\n",
       " ('nekrasov', 0.7298333644866943),\n",
       " ('ooguri', 0.7281148433685303),\n",
       " ('donaldson', 0.7122985124588013),\n",
       " ('dougla', 0.7082318067550659),\n",
       " ('refin', 0.7049825191497803),\n",
       " ('thoma', 0.6772043704986572),\n",
       " ('lmo', 0.6763520240783691),\n",
       " ('quiver', 0.668645977973938),\n",
       " ('gaiotto', 0.6548724174499512),\n",
       " ('equivari', 0.649773120880127)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_hepmodel.wv.most_similar(positive=[\"witten\"],topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(w1,w2):\n",
    "    print(f\"Random distance : {randomdistance(w1,w2)} \")    \n",
    "    print(f\"Word2Vec distance : {olddistance(w1,w2)} \")\n",
    "    print(f\"Our LSTM distance : {newdistance(w1,w2)} \")\n",
    "def pickledistance(w1,w2,new_weights):\n",
    "    num=np.dot(new_weights[short_hepmodel.wv.vocab.get(w1).index,:],new_weights[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    den1=np.dot(new_weights[short_hepmodel.wv.vocab.get(w1).index,:],new_weights[short_hepmodel.wv.vocab.get(w1).index,:])\n",
    "    den2=np.dot(new_weights[short_hepmodel.wv.vocab.get(w2).index,:],new_weights[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    newdist= num/np.sqrt(den1*den2)\n",
    "    num=np.dot(short_hepmodel.wv[w1].T,short_hepmodel.wv[w2])\n",
    "    den1=np.dot(short_hepmodel.wv[w1].T,short_hepmodel.wv[w1])\n",
    "    den2=np.dot(short_hepmodel.wv[w2].T,short_hepmodel.wv[w2])\n",
    "    olddist= num/np.sqrt(den1*den2)\n",
    "    num=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    den1=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w1).index,:])\n",
    "    den2=np.dot(kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:],kerasvectors[short_hepmodel.wv.vocab.get(w2).index,:])\n",
    "    rand= num/np.sqrt(den1*den2)\n",
    "    print(f\"Random distance : {rand} \")    \n",
    "    print(f\"Word2Vec distance : {olddist} \")\n",
    "    print(f\"Our LSTM distance : {newdist} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.8078928351549151 \n",
      "Word2Vec distance : 0.8078927397727966 \n",
      "Our LSTM distance : 0.5743873715400696 \n"
     ]
    }
   ],
   "source": [
    "distance(\"witten\",\"seiberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.4012932769746991 \n",
      "Word2Vec distance : 0.4012932777404785 \n",
      "Our LSTM distance : 0.24563710391521454 \n"
     ]
    }
   ],
   "source": [
    "distance(\"gener\",\"relat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.11760112770347647 \n",
      "Word2Vec distance : 0.11760114133358002 \n",
      "Our LSTM distance : 0.14298424124717712 \n"
     ]
    }
   ],
   "source": [
    "distance(\"witten\",\"brane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the word vectors to gensim #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_file = datapath('/home/ubuntu/project/LSTM/Mar4bigtrainingWord/Weights_v3.txt')\n",
    "tmp_file = get_tmpfile(\"/home/ubuntu/project/LSTM/Mar4bigtrainingWord/Weights_v3_gensim.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_model = KeyedVectors.load_word2vec_format(tmp_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('threemanifold', 0.5747983455657959),\n",
       " ('einsenstein', 0.5551432371139526),\n",
       " ('polem', 0.554176926612854),\n",
       " ('central', 0.549871563911438),\n",
       " ('superconform', 0.5214196443557739),\n",
       " ('truism', 0.5194559693336487),\n",
       " ('troessaert', 0.5095899105072021),\n",
       " ('teichner', 0.5086613893508911),\n",
       " ('notivarg', 0.5060545802116394),\n",
       " ('asep', 0.503456711769104),\n",
       " ('talli', 0.49898064136505127),\n",
       " ('reshap', 0.4967975914478302),\n",
       " ('poincar', 0.49543410539627075),\n",
       " ('contradistinct', 0.49385619163513184),\n",
       " ('pankiewicz', 0.49157631397247314)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model.similar_by_word(\"conform\",topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making abstract vectors #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n"
     ]
    }
   ],
   "source": [
    "sent=np.zeros((134296,155,52))\n",
    "for i in range(134296):\n",
    "    sent[i]=our_model.wv.vectors[train_X_indices[i]]\n",
    "    if i%1000==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0905.1699'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iden=hep_abstracts_limit['id'].values\n",
    "iden[10343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "senter=np.average(sent,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the abstract vectors to the file with a keyword that is given by the arxiv ID #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"sentences_fromwords_v3.txt\", \"w\")\n",
    "for i in range(134296):\n",
    "    f.write(f\"{iden[i]} \")\n",
    "    x=senter[i]\n",
    "    f.write((' '.join(['%0.10f']*x.size)+'\\n') % tuple(x))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(our_model,open(\"modelv3\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now test them #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "g = open(\"/home/ubuntu/project/LSTM/Mar4bigtrainingWord/sentences_fromwords_v3_gensim.txt\", \"w\")\n",
    "glove_file = datapath('/home/ubuntu/project/LSTM/Mar4bigtrainingWord/sentences_fromwords_v3.txt')\n",
    "tmp_file = get_tmpfile(\"/home/ubuntu/project/LSTM/Mar4bigtrainingWord/sentences_fromwords_v3_gensim.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "average_sentence_model = KeyedVectors.load_word2vec_format(tmp_file,)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below code makes paper recommendations using gensim's most similar function#\n",
    "\n",
    "## It also measure the TF-IDF cosine distance and rejects the recommendation if the distance is less than 0.1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tij=hep_abstracts_limit\n",
    "tij = tij.reset_index(drop=True)\n",
    "def fastsimplerecommend(string,string1):\n",
    "    pointer3=hep_abstracts.loc[(hep_abstracts['id'].str.match(string))]\n",
    "    queryvalue3=pointer3['abstract'].values\n",
    "    processed_query3=preprocess_documents(queryvalue3)\n",
    "    test_bow_corpus3 = [hep_dictionary_lim.doc2bow(text) for text in processed_query3]\n",
    "    vec_test_bow3=hep_tfidf[test_bow_corpus3]\n",
    "    sims = index[vec_test_bow3[0]]\n",
    "    ind=tij[tij['id'] == string1].index[0]\n",
    "    #print(ind)\n",
    "    if sims[ind]>0.1:\n",
    "        return (1,sims[ind])\n",
    "    return (0,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1611.07240', 0.9957449436187744),\n",
       " ('1007.3865', 0.9955410957336426),\n",
       " ('0808.2605', 0.995360255241394),\n",
       " ('1311.1160', 0.9953429698944092),\n",
       " ('1108.1180', 0.9952816367149353),\n",
       " ('hep-th/0003247', 0.9950817823410034),\n",
       " ('1510.07770', 0.9950804710388184),\n",
       " ('1904.02038', 0.9950685501098633),\n",
       " ('hep-ph/0011133', 0.9949870109558105),\n",
       " ('hep-ph/9812529', 0.9949166774749756),\n",
       " ('1005.3016', 0.9949142336845398),\n",
       " ('1911.00496', 0.994881808757782),\n",
       " ('hep-th/9803094', 0.9948656558990479),\n",
       " ('hep-ph/0007032', 0.9948521852493286),\n",
       " ('hep-ph/0505038', 0.9948427081108093)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_sentence_model.similar_by_word(\"1908.00015\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastlstmrecommender(orig):\n",
    "    i=1\n",
    "    j=0\n",
    "    starter=average_sentence_model.similar_by_word(orig,topn=1000)\n",
    "    while i<31:\n",
    "        x=starter[j][0]\n",
    "        #print(orig,string)\n",
    "        if fastsimplerecommend(orig,x)[0]==1:\n",
    "            print(f\"\\n {i}) With TFIDF score {fastsimplerecommend(orig,x)[1]} and GPT-2 rank {j} \")\n",
    "            print(f\" Paper title : {hep_abstracts.loc[hep_abstracts['id'] == x].title.values}. \\n Abstract : {hep_abstracts.loc[hep_abstracts['id'] == x].abstract.values}. \\n Authors : {datapd_hep_th.loc[hep_abstracts['id'] == x].authors.values}. \\n Arxiv ID : {hep_abstracts.loc[hep_abstracts['id'] == x].id.values} \\n \\n \")\n",
    "            i=i+1\n",
    "        j=j+1\n",
    "def lambdalstmrecommender(orig):\n",
    "    i=1\n",
    "    j=0\n",
    "    starter=average_sentence_model.similar_by_word(orig,topn=1000)\n",
    "    while i<31:\n",
    "        x=starter[j][0]\n",
    "        #print(orig,string)\n",
    "        if fastsimplerecommend(orig,x)[0]==1:\n",
    "            print(f\"\\\"{hep_abstracts.loc[hep_abstracts['id'] == x].id.values[0]}\\\",\",) \n",
    "            i=i+1\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"hep-th/0105283\",\n",
      "\"1802.04390\",\n",
      "\"0912.2726\",\n",
      "\"1910.07424\",\n",
      "\"2005.09105\",\n",
      "\"1809.00551\",\n",
      "\"1712.06622\",\n",
      "\"1211.5838\",\n",
      "\"1702.08471\",\n",
      "\"hep-th/0601148\",\n",
      "\"2003.07361\",\n",
      "\"hep-th/0201258\",\n",
      "\"1203.5129\",\n",
      "\"1911.11487\",\n",
      "\"1808.08155\",\n",
      "\"1207.3112\",\n",
      "\"1811.00467\",\n",
      "\"1812.06479\",\n",
      "\"1310.5078\",\n",
      "\"1709.00008\",\n",
      "\"1306.1930\",\n",
      "\"1504.00772\",\n",
      "\"1408.3629\",\n",
      "\"1512.05362\",\n",
      "\"1411.7941\",\n",
      "\"0905.2211\",\n",
      "\"hep-th/9310130\",\n",
      "\"1404.5864\",\n",
      "\"1803.05086\",\n",
      "\"hep-th/0409058\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1212.3616\") #Jared et al "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1805.12145\",\n",
      "\"1303.1884\",\n",
      "\"1803.06164\",\n",
      "\"1403.3416\",\n",
      "\"1408.6932\",\n",
      "\"1806.02209\",\n",
      "\"1705.08319\",\n",
      "\"1807.04294\",\n",
      "\"1404.3105\",\n",
      "\"1310.1839\",\n",
      "\"1705.01561\",\n",
      "\"1503.03542\",\n",
      "\"hep-th/9809035\",\n",
      "\"1412.8465\",\n",
      "\"1912.04837\",\n",
      "\"hep-th/0203219\",\n",
      "\"1410.5763\",\n",
      "\"1109.0544\",\n",
      "\"hep-th/0002145\",\n",
      "\"0804.0632\",\n",
      "\"1709.03205\",\n",
      "\"hep-th/9901148\",\n",
      "\"1504.00106\",\n",
      "\"1902.02504\",\n",
      "\"1201.1702\",\n",
      "\"1411.1887\",\n",
      "\"hep-th/0201036\",\n",
      "\"0711.1221\",\n",
      "\"2009.11595\",\n",
      "\"1010.3700\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1304.4926\") #Lewkowycz Maldacena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"hep-th/0610140\",\n",
      "\"1608.08304\",\n",
      "\"1911.09666\",\n",
      "\"2004.13857\",\n",
      "\"1205.5036\",\n",
      "\"0806.4373\",\n",
      "\"1412.0802\",\n",
      "\"1512.06855\",\n",
      "\"1305.3448\",\n",
      "\"0704.0507\",\n",
      "\"gr-qc/0505068\",\n",
      "\"hep-th/0605068\",\n",
      "\"1402.5127\",\n",
      "\"hep-th/9607235\",\n",
      "\"1305.3757\",\n",
      "\"1502.00583\",\n",
      "\"0705.3150\",\n",
      "\"1905.04317\",\n",
      "\"hep-th/0606069\",\n",
      "\"1612.04857\",\n",
      "\"hep-th/0011286\",\n",
      "\"0911.5070\",\n",
      "\"1603.07976\",\n",
      "\"1912.02210\",\n",
      "\"quant-ph/0311049\",\n",
      "\"0804.2123\",\n",
      "\"gr-qc/9909061\",\n",
      "\"2010.07756\",\n",
      "\"1406.1804\",\n",
      "\"hep-th/0302170\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1908.10996\") #Maldacena et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"2010.04726\",\n",
      "\"1803.04938\",\n",
      "\"1005.4516\",\n",
      "\"1609.02165\",\n",
      "\"1610.05308\",\n",
      "\"hep-th/9403026\",\n",
      "\"1611.00763\",\n",
      "\"2005.06343\",\n",
      "\"1909.05775\",\n",
      "\"hep-th/9604044\",\n",
      "\"1403.1622\",\n",
      "\"1012.3740\",\n",
      "\"1711.04371\",\n",
      "\"1807.05941\",\n",
      "\"1804.09334\",\n",
      "\"1801.04208\",\n",
      "\"1611.05577\",\n",
      "\"1912.04105\",\n",
      "\"1308.1726\",\n",
      "\"1810.07199\",\n",
      "\"2011.09250\",\n",
      "\"1206.3129\",\n",
      "\"1512.02532\",\n",
      "\"hep-th/9411100\",\n",
      "\"1504.01737\",\n",
      "\"1812.02226\",\n",
      "\"2002.03865\",\n",
      "\"hep-th/0109064\",\n",
      "\"1212.4103\",\n",
      "\"1601.02883\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1805.00098\") #DSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1611.09381\",\n",
      "\"2009.03948\",\n",
      "\"1005.1973\",\n",
      "\"hep-th/0608182\",\n",
      "\"1712.09826\",\n",
      "\"1511.06842\",\n",
      "\"1307.7732\",\n",
      "\"hep-th/0506177\",\n",
      "\"1410.4466\",\n",
      "\"1012.1040\",\n",
      "\"1106.2291\",\n",
      "\"1808.08239\",\n",
      "\"1310.4549\",\n",
      "\"1704.06076\",\n",
      "\"1411.6405\",\n",
      "\"1408.2538\",\n",
      "\"1912.12996\",\n",
      "\"1511.08816\",\n",
      "\"1809.04081\",\n",
      "\"1606.04123\",\n",
      "\"1303.7211\",\n",
      "\"hep-th/9805146\",\n",
      "\"1602.07177\",\n",
      "\"1105.4862\",\n",
      "\"hep-th/0112058\",\n",
      "\"hep-th/0607111\",\n",
      "\"0812.2234\",\n",
      "\"hep-th/0703046\",\n",
      "\"1203.2636\",\n",
      "\"1108.1205\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1606.01857\") #Yang et al "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1307.7732\",\n",
      "\"hep-th/9504033\",\n",
      "\"0711.0133\",\n",
      "\"0910.5596\",\n",
      "\"1504.05044\",\n",
      "\"0706.0610\",\n",
      "\"hep-th/0312038\",\n",
      "\"1411.6505\",\n",
      "\"1604.00383\",\n",
      "\"1703.10178\",\n",
      "\"hep-th/9412209\",\n",
      "\"2011.06026\",\n",
      "\"hep-th/0210017\",\n",
      "\"hep-th/0203219\",\n",
      "\"1908.02470\",\n",
      "\"2009.03948\",\n",
      "\"1912.00029\",\n",
      "\"hep-th/9803180\",\n",
      "\"1803.10775\",\n",
      "\"hep-th/9204072\",\n",
      "\"1404.0033\",\n",
      "\"hep-th/0412223\",\n",
      "\"1507.04757\",\n",
      "\"1403.2721\",\n",
      "\"hep-th/0610163\",\n",
      "\"0802.2257\",\n",
      "\"1209.5396\",\n",
      "\"hep-th/0506176\",\n",
      "\"hep-th/0301239\",\n",
      "\"hep-th/0003144\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"0712.0155\") # Maloney Witten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1510.08772\",\n",
      "\"1905.01355\",\n",
      "\"1904.00032\",\n",
      "\"2006.08221\",\n",
      "\"1510.02464\",\n",
      "\"0901.4864\",\n",
      "\"1706.02388\",\n",
      "\"1802.06889\",\n",
      "\"0704.1417\",\n",
      "\"1407.6429\",\n",
      "\"1806.08181\",\n",
      "\"1910.04661\",\n",
      "\"1611.05797\",\n",
      "\"1305.4604\",\n",
      "\"1002.4613\",\n",
      "\"1402.6127\",\n",
      "\"1104.4783\",\n",
      "\"1210.7705\",\n",
      "\"1706.08456\",\n",
      "\"1605.06039\",\n",
      "\"2007.11647\",\n",
      "\"2002.12091\",\n",
      "\"hep-lat/9709079\",\n",
      "\"1310.4778\",\n",
      "\"hep-th/9903196\",\n",
      "\"hep-ph/0306250\",\n",
      "\"1302.3539\",\n",
      "\"1605.08087\",\n",
      "\"1603.05150\",\n",
      "\"1607.03123\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1703.00278\") # Inversion formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1412.1276\",\n",
      "\"1505.04804\",\n",
      "\"1511.05179\",\n",
      "\"1406.0520\",\n",
      "\"0704.0140\",\n",
      "\"1608.00328\",\n",
      "\"1407.6429\",\n",
      "\"1307.6607\",\n",
      "\"1107.2370\",\n",
      "\"1711.09913\",\n",
      "\"1310.3676\",\n",
      "\"hep-th/0105117\",\n",
      "\"2010.00407\",\n",
      "\"1605.02835\",\n",
      "\"1811.11189\",\n",
      "\"hep-th/0101083\",\n",
      "\"1606.04537\",\n",
      "\"hep-th/9907129\",\n",
      "\"cond-mat/0008216\",\n",
      "\"1306.4974\",\n",
      "\"1506.03772\",\n",
      "\"1303.0741\",\n",
      "\"1405.0015\",\n",
      "\"0907.5542\",\n",
      "\"1412.5205\",\n",
      "\"hep-th/0002250\",\n",
      "\"1108.0677\",\n",
      "\"1604.07830\",\n",
      "\"1303.6955\",\n",
      "\"1608.01283\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1405.5137\") # HKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1701.06340\",\n",
      "\"1102.1734\",\n",
      "\"1808.07760\",\n",
      "\"1910.06675\",\n",
      "\"hep-th/0112150\",\n",
      "\"1909.11402\",\n",
      "\"1801.02714\",\n",
      "\"1109.0544\",\n",
      "\"1612.00141\",\n",
      "\"1806.10807\",\n",
      "\"0812.0152\",\n",
      "\"1106.1533\",\n",
      "\"1809.04081\",\n",
      "\"2004.01192\",\n",
      "\"1602.06786\",\n",
      "\"hep-th/0602215\",\n",
      "\"1401.4998\",\n",
      "\"1811.07965\",\n",
      "\"2011.04664\",\n",
      "\"1609.08912\",\n",
      "\"1806.00306\",\n",
      "\"1804.07544\",\n",
      "\"1609.06300\",\n",
      "\"1909.02357\",\n",
      "\"1211.4506\",\n",
      "\"hep-th/0311004\",\n",
      "\"1707.09663\",\n",
      "\"hep-th/0505046\",\n",
      "\"hep-th/0101118\",\n",
      "\"1210.0284\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1611.03470\") # Verlinde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0812.3903\",\n",
      "\"cond-mat/0311222\",\n",
      "\"0705.3388\",\n",
      "\"1206.6375\",\n",
      "\"0708.3199\",\n",
      "\"1501.07615\",\n",
      "\"2008.13446\",\n",
      "\"hep-th/0311177\",\n",
      "\"1808.07056\",\n",
      "\"0906.4959\",\n",
      "\"1102.1900\",\n",
      "\"hep-ph/0206181\",\n",
      "\"2008.02579\",\n",
      "\"1108.0683\",\n",
      "\"1603.00553\",\n",
      "\"1003.1302\",\n",
      "\"1707.05307\",\n",
      "\"1302.4898\",\n",
      "\"1409.8305\",\n",
      "\"1501.04033\",\n",
      "\"1709.01801\",\n",
      "\"1002.3823\",\n",
      "\"1004.3570\",\n",
      "\"1910.12856\",\n",
      "\"1406.5498\",\n",
      "\"hep-th/0505038\",\n",
      "\"hep-th/9512048\",\n",
      "\"1410.3390\",\n",
      "\"0806.0558\",\n",
      "\"1706.02702\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"hep-th/0310285\")  #Confinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1906.05501\",\n",
      "\"1811.05171\",\n",
      "\"2008.01755\",\n",
      "\"1408.3705\",\n",
      "\"1906.03669\",\n",
      "\"1912.02799\",\n",
      "\"1909.11402\",\n",
      "\"1606.04537\",\n",
      "\"2004.15010\",\n",
      "\"2003.11870\",\n",
      "\"1912.05649\",\n",
      "\"0907.0151\",\n",
      "\"1806.10560\",\n",
      "\"1902.10161\",\n",
      "\"2010.01907\",\n",
      "\"1504.06632\",\n",
      "\"1407.2900\",\n",
      "\"1612.04373\",\n",
      "\"1507.00779\",\n",
      "\"2005.05962\",\n",
      "\"1809.01355\",\n",
      "\"1702.01748\",\n",
      "\"1607.03605\",\n",
      "\"1612.00017\",\n",
      "\"1810.02055\",\n",
      "\"1703.07780\",\n",
      "\"1903.11244\",\n",
      "\"1209.4641\",\n",
      "\"2005.05971\",\n",
      "\"0904.3544\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1411.7041\")  #Dong 1411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1310.6334\",\n",
      "\"1812.05268\",\n",
      "\"1808.05963\",\n",
      "\"1604.01772\",\n",
      "\"1906.04745\",\n",
      "\"1704.00187\",\n",
      "\"hep-th/0209189\",\n",
      "\"1905.06920\",\n",
      "\"1408.0415\",\n",
      "\"1512.06431\",\n",
      "\"1803.10539\",\n",
      "\"1306.4324\",\n",
      "\"1605.05751\",\n",
      "\"1403.1393\",\n",
      "\"1811.03113\",\n",
      "\"1311.4516\",\n",
      "\"1703.05445\",\n",
      "\"0909.5617\",\n",
      "\"1703.01759\",\n",
      "\"2011.00407\",\n",
      "\"0801.2863\",\n",
      "\"1805.05398\",\n",
      "\"1511.07194\",\n",
      "\"1701.02319\",\n",
      "\"1703.09222\",\n",
      "\"1908.02044\",\n",
      "\"1606.07628\",\n",
      "\"2007.05365\",\n",
      "\"1601.07616\",\n",
      "\"1802.01040\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1601.05416\")  #Dong 1601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1802.10103\",\n",
      "\"1803.01874\",\n",
      "\"1705.01486\",\n",
      "\"2008.07532\",\n",
      "\"1604.05308\",\n",
      "\"1705.08453\",\n",
      "\"1607.07506\",\n",
      "\"1611.03470\",\n",
      "\"2010.15813\",\n",
      "\"1703.00456\",\n",
      "\"2004.07870\",\n",
      "\"1809.00026\",\n",
      "\"1511.06713\",\n",
      "\"1909.11402\",\n",
      "\"2004.07242\",\n",
      "\"1611.01846\",\n",
      "\"1412.0687\",\n",
      "\"1806.10807\",\n",
      "\"1811.05382\",\n",
      "\"1407.8171\",\n",
      "\"1802.04278\",\n",
      "\"0907.1625\",\n",
      "\"2006.01835\",\n",
      "\"2006.10740\",\n",
      "\"1512.08233\",\n",
      "\"math-ph/9805013\",\n",
      "\"1908.09939\",\n",
      "\"1707.03825\",\n",
      "\"2005.05962\",\n",
      "\"1406.2991\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1605.08072\")  #Faulkner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1412.1276\",\n",
      "\"1505.04804\",\n",
      "\"1511.05179\",\n",
      "\"1406.0520\",\n",
      "\"0704.0140\",\n",
      "\"1608.00328\",\n",
      "\"1407.6429\",\n",
      "\"1307.6607\",\n",
      "\"1107.2370\",\n",
      "\"1711.09913\",\n",
      "\"1310.3676\",\n",
      "\"hep-th/0105117\",\n",
      "\"2010.00407\",\n",
      "\"1605.02835\",\n",
      "\"1811.11189\",\n",
      "\"hep-th/0101083\",\n",
      "\"1606.04537\",\n",
      "\"hep-th/9907129\",\n",
      "\"cond-mat/0008216\",\n",
      "\"1306.4974\",\n",
      "\"1506.03772\",\n",
      "\"1303.0741\",\n",
      "\"1405.0015\",\n",
      "\"0907.5542\",\n",
      "\"1412.5205\",\n",
      "\"hep-th/0002250\",\n",
      "\"1108.0677\",\n",
      "\"1604.07830\",\n",
      "\"1303.6955\",\n",
      "\"1608.01283\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1405.5137\")  #HKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1302.5060\",\n",
      "\"1206.6106\",\n",
      "\"1910.04484\",\n",
      "\"1012.1040\",\n",
      "\"1406.2328\",\n",
      "\"1306.4501\",\n",
      "\"1609.08912\",\n",
      "\"1310.0819\",\n",
      "\"1704.07410\",\n",
      "\"1405.3965\",\n",
      "\"1912.05746\",\n",
      "\"1106.1533\",\n",
      "\"1805.00284\",\n",
      "\"1806.00306\",\n",
      "\"0809.2596\",\n",
      "\"1704.05141\",\n",
      "\"0809.4928\",\n",
      "\"1704.06076\",\n",
      "\"2008.13699\",\n",
      "\"0812.1818\",\n",
      "\"1312.4953\",\n",
      "\"hep-th/0306031\",\n",
      "\"hep-th/0303174\",\n",
      "\"gr-qc/0612191\",\n",
      "\"1311.4794\",\n",
      "\"1403.7639\",\n",
      "\"1304.6305\",\n",
      "\"1912.09402\",\n",
      "\"1710.04221\",\n",
      "\"hep-th/0610113\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"0712.2456\")  #fluid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1905.08256\",\n",
      "\"1205.6157\",\n",
      "\"2007.12188\",\n",
      "\"1610.01569\",\n",
      "\"1902.07407\",\n",
      "\"1707.01740\",\n",
      "\"1706.02231\",\n",
      "\"hep-th/9711167\",\n",
      "\"1508.06668\",\n",
      "\"1708.06446\",\n",
      "\"hep-th/9305031\",\n",
      "\"cond-mat/0701501\",\n",
      "\"hep-ph/0510012\",\n",
      "\"1905.00026\",\n",
      "\"1802.00364\",\n",
      "\"gr-qc/0212003\",\n",
      "\"1410.0298\",\n",
      "\"1903.00478\",\n",
      "\"1701.07426\",\n",
      "\"1711.11042\",\n",
      "\"1905.07694\",\n",
      "\"hep-th/9402109\",\n",
      "\"2006.07317\",\n",
      "\"1705.03956\",\n",
      "\"hep-th/9612053\",\n",
      "\"1808.07455\",\n",
      "\"1407.4497\",\n",
      "\"2008.03308\",\n",
      "\"hep-lat/9707027\",\n",
      "\"hep-th/9703107\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1604.07818\")  #SYK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1907.04289\",\n",
      "\"1706.07439\",\n",
      "\"1611.04592\",\n",
      "\"1809.07228\",\n",
      "\"1805.01051\",\n",
      "\"1605.08124\",\n",
      "\"0712.3574\",\n",
      "\"1809.01671\",\n",
      "\"1711.05556\",\n",
      "\"1812.04770\",\n",
      "\"1211.3425\",\n",
      "\"2009.00069\",\n",
      "\"1506.04795\",\n",
      "\"1806.06840\",\n",
      "\"1708.08822\",\n",
      "\"hep-th/0201165\",\n",
      "\"1702.04350\",\n",
      "\"1802.06796\",\n",
      "\"1209.2417\",\n",
      "\"1410.3472\",\n",
      "\"1701.07783\",\n",
      "\"1206.2630\",\n",
      "\"0808.0530\",\n",
      "\"1705.07597\",\n",
      "\"hep-th/0508044\",\n",
      "\"hep-ph/0006333\",\n",
      "\"hep-th/0003208\",\n",
      "\"1304.0007\",\n",
      "\"1110.0803\",\n",
      "\"0808.1725\",\n"
     ]
    }
   ],
   "source": [
    "lambdalstmrecommender(\"1611.04650\") # Saad Cotler Phil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human checks #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1) With TFIDF score 0.32867228984832764 and GPT-2 rank 0 \n",
      " Paper title : ['Operator product expansion in SL(2) conformal field theory']. \n",
      " Abstract : ['  In the conformal field theories having affine SL(2) symmetry, we study the\\noperator product expansion (OPE) involving primary fields in highest weight\\nrepresentations. For this purpose, we analyze properties of primary fields with\\ndefinite SL(2) weights, and calculate their two- and three-point functions.\\nUsing these correlators, we show that the correct OPE is obtained when one of\\nthe primary fields belongs to the degenerate highest weight representation. We\\nbriefly comment on the OPE in the SL(2,R) WZNW model.\\n']. \n",
      " Authors : ['Kazuo Hosomichi and Yuji Satoh']. \n",
      " Arxiv ID : ['hep-th/0105283'] \n",
      " \n",
      " \n",
      "\n",
      " 2) With TFIDF score 0.45168179273605347 and GPT-2 rank 1 \n",
      " Paper title : ['Veneziano Amplitude of Vasiliev Theory']. \n",
      " Abstract : [\"  We compute the four-point function of scalar operators in CFTs with weakly\\nbroken higher spin symmetry at arbitrary 't Hooft coupling. We use the known\\nthree-point functions in these theories, the Lorentzian OPE inversion formula\\nand crossing to fix the result up to the addition of three functions of the\\ncross ratios. These are given by contact Witten diagrams in AdS and manifest\\nnon-analyticity of the OPE data in spin. We use Schwinger-Dyson equations to\\nshow that such terms are absent in the large $N$ Chern-Simons matter theories.\\nThe result is that the OPE data is analytic in spin up to $J=0$.\\n\"]. \n",
      " Authors : ['Gustavo J. Turiaci and Alexander Zhiboedov']. \n",
      " Arxiv ID : ['1802.04390'] \n",
      " \n",
      " \n",
      "\n",
      " 3) With TFIDF score 0.392424613237381 and GPT-2 rank 2 \n",
      " Paper title : ['Rigorous Limits on the Interaction Strength in Quantum Field Theory']. \n",
      " Abstract : ['  We derive model-independent, universal upper bounds on the Operator Product\\nExpansion (OPE) coefficients in unitary 4-dimensional Conformal Field Theories.\\nThe method uses the conformal block decomposition and the crossing symmetry\\nconstraint of the 4-point function. In particular, the OPE coefficient of three\\nidentical dimension $d$ scalar primaries is found to be bounded by ~ 10(d-1)\\nfor 1<d<1.7. This puts strong limits on unparticle self-interaction cross\\nsections at the LHC.\\n']. \n",
      " Authors : ['Francesco Caracciolo, Slava Rychkov']. \n",
      " Arxiv ID : ['0912.2726'] \n",
      " \n",
      " \n",
      "\n",
      " 4) With TFIDF score 0.2861548960208893 and GPT-2 rank 3 \n",
      " Paper title : ['Celestial Operator Products of Gluons and Gravitons']. \n",
      " Abstract : ['  The operator product expansion (OPE) on the celestial sphere of conformal\\nprimary gluons and gravitons is studied. Asymptotic symmetries imply recursion\\nrelations between products of operators whose conformal weights differ by\\nhalf-integers. It is shown, for tree-level Einstein-Yang-Mills theory, that\\nthese recursion relations are so constraining that they completely fix the\\nleading celestial OPE coefficients in terms of the Euler beta function. The\\npoles in the beta functions are associated with conformally soft currents.\\n']. \n",
      " Authors : ['Monica Pate, Ana-Maria Raclariu, Andrew Strominger and Ellis Ye Yuan']. \n",
      " Arxiv ID : ['1910.07424'] \n",
      " \n",
      " \n",
      "\n",
      " 5) With TFIDF score 0.35278552770614624 and GPT-2 rank 5 \n",
      " Paper title : ['Classification of Convergent OPE Channels for Lorentzian CFT Four-Point\\n  Functions']. \n",
      " Abstract : ['  We analyze the convergence properties of operator product expansions (OPE)\\nfor Lorentzian CFT four-point functions of scalar operators. We give a complete\\nclassification of Lorentzian four-point configurations. All configurations in\\neach class have the same OPE convergence properties in s-, t- and u-channels.\\nWe give tables including the information of OPE convergence for all classes.\\nOur work justifies that in a subset of the configuration space, Lorentzian CFT\\nfour-point functions are genuine analytic functions. Our results are valid for\\nunitary CFTs in $d\\\\geq2$. Our work also provides some Lorentzian regions where\\none can do bootstrap analysis in the sense of functions.\\n']. \n",
      " Authors : ['Jiaxin Qiao']. \n",
      " Arxiv ID : ['2005.09105'] \n",
      " \n",
      " \n",
      "\n",
      " 6) With TFIDF score 0.3138936758041382 and GPT-2 rank 7 \n",
      " Paper title : ['All five-loop planar four-point functions of half-BPS operators in\\n  $\\\\mathcal N=4$ SYM']. \n",
      " Abstract : ['  We obtain all planar four-point correlators of half-BPS operators in\\n$\\\\mathcal{N}=4$ SYM up to five loops. The ansatz for the integrand is fixed\\npartially by imposing light-cone OPE relations between different correlators.\\nWe then fix the integrated correlators by comparing their asymptotic expansions\\nwith simple data obtained from integrability. We extract OPE coefficients and\\nfind a prediction for the triple wrapping correction of the hexagon form\\nfactors, which contributes already at the five-loop order.\\n']. \n",
      " Authors : ['Dmitry Chicherin, Alessandro Georgoudis, Vasco Goncalves and Raul\\n  Pereira']. \n",
      " Arxiv ID : ['1809.00551'] \n",
      " \n",
      " \n",
      "\n",
      " 7) With TFIDF score 0.32947805523872375 and GPT-2 rank 15 \n",
      " Paper title : ['Boundary multi-trace deformations and OPEs in AdS/CFT correspondence']. \n",
      " Abstract : ['  We argue that multi-trace deformations of the boundary CFT in AdS/CFT\\ncorrespondence can arise through the OPE of single-trace operators. We work out\\nthe example of a scalar field in AdS_5 with cubic self interaction. By an\\nappropriate reparametrization of the boundary data we are able to deform the\\nboundary CFT by a marginal operator that couples to the conformal anomaly. Our\\nmethod can be used in the analysis of multi-trace deformations in N=4 SYM where\\nthe OPEs of various single-trace operators are known.\\n']. \n",
      " Authors : ['A. C. Petkou']. \n",
      " Arxiv ID : ['hep-th/0201258'] \n",
      " \n",
      " \n",
      "\n",
      " 8) With TFIDF score 0.4431057572364807 and GPT-2 rank 16 \n",
      " Paper title : ['Bounds on SCFTs from Conformal Perturbation Theory']. \n",
      " Abstract : ['  The operator product expansion (OPE) in 4d (super)conformal field theory is\\nof broad interest, for both formal and phenomenological applications. In this\\npaper, we use conformal perturbation theory to study the OPE of nearly-free\\nfields coupled to SCFTs. Under fairly general assumptions, we show that the OPE\\nof a chiral operator of dimension $\\\\Delta = 1+\\\\epsilon$ with its complex\\nconjugate always contains an operator of dimension less than $2 \\\\Delta$. Our\\nbounds apply to Banks-Zaks fixed points and their generalizations, as we\\nillustrate using several examples.\\n']. \n",
      " Authors : ['Daniel Green and David Shih']. \n",
      " Arxiv ID : ['1203.5129'] \n",
      " \n",
      " \n",
      "\n",
      " 9) With TFIDF score 0.25376299023628235 and GPT-2 rank 19 \n",
      " Paper title : ['Analytic Bootstrap for Boundary CFT']. \n",
      " Abstract : ['  We propose a method to analytically solve the bootstrap equation for two\\npoint functions in boundary CFT. We consider the analytic structure of the\\ncorrelator in Lorentzian signature and in particular the discontinuity of bulk\\nand boundary conformal blocks to extract CFT data. As an application, the\\ncorrelator $\\\\langle \\\\phi \\\\phi \\\\rangle$ in $\\\\phi^4$ theory at the Wilson-Fisher\\nfixed point is computed to order $\\\\epsilon^2$ in the $\\\\epsilon$ expansion.\\n']. \n",
      " Authors : ['Agnese Bissi, Tobias Hansen, Alexander S\\\\\"oderberg']. \n",
      " Arxiv ID : ['1808.08155'] \n",
      " \n",
      " \n",
      "\n",
      " 10) With TFIDF score 0.3191680610179901 and GPT-2 rank 20 \n",
      " Paper title : ['Three-loop universal structure constants in N=4 susy Yang-Mills theory']. \n",
      " Abstract : ['  We present a conjecture for the normalisation of the twist two conformal\\npartial waves in a double OPE limit of the four-point function of stress tensor\\nmultiplets in N = 4 super Yang-Mills theory up to three loops. This contains\\ninformation about the structure constants in the OPE. Like the twist two\\nanomalous dimensions our result is expressed as a linear combination of\\nharmonic sums whose argument is the spin of the exchanged operators. To arrive\\nat the result we derive asymptotic expansions for the twist two part of two\\nunknown three-loop integrals using the method of expansion by regions,\\ncomplemented by some intuition gained on the example of the ladder integrals up\\nto three loops.\\n']. \n",
      " Authors : ['Burkhard Eden']. \n",
      " Arxiv ID : ['1207.3112'] \n",
      " \n",
      " \n",
      "\n",
      " 11) With TFIDF score 0.2358129322528839 and GPT-2 rank 23 \n",
      " Paper title : ['Holographic correlators in AdS$_3$']. \n",
      " Abstract : ['  We derive the four-point correlators of scalar operators of dimension one in\\nthe supergravity limit of the D1D5 CFT holographically dual to string theory on\\nAdS$_3\\\\times S^3\\\\times \\\\mathcal{M}$, with $\\\\mathcal{M}$ either $T^4$ or $K3$.\\nWe avoid the use of Witten diagrams but deduce our result from a limit of the\\nheavy-heavy-light-light correlators computed in arXiv:1705.09250, together with\\nseveral consistency requirements of the OPE in the various channels. This\\nresult represents the first holographic correlators of single-trace operators\\ncomputed in AdS$_3$.\\n']. \n",
      " Authors : ['Stefano Giusto, Rodolfo Russo, Congkao Wen']. \n",
      " Arxiv ID : ['1812.06479'] \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 12) With TFIDF score 0.33895793557167053 and GPT-2 rank 24 \n",
      " Paper title : ['Bootstrapping the 3d Ising twist defect']. \n",
      " Abstract : ['  Recent numerical results point to the existence of a conformally invariant\\ntwist defect in the critical 3d Ising model. In this note we show that this\\nfact is supported by both epsilon expansion and conformal bootstrap\\ncalculations. We find that our results are in good agreement with the numerical\\ndata. We also make new predictions for operator dimensions and OPE coefficients\\nfrom the bootstrap approach. In the process we derive universal bounds on\\none-dimensional conformal field theories and conformal line defects.\\n']. \n",
      " Authors : ['Davide Gaiotto, Dalimil Mazac, Miguel F. Paulos']. \n",
      " Arxiv ID : ['1310.5078'] \n",
      " \n",
      " \n",
      "\n",
      " 13) With TFIDF score 0.20933914184570312 and GPT-2 rank 25 \n",
      " Paper title : ['A tauberian theorem for the conformal bootstrap']. \n",
      " Abstract : ['  For expansions in one-dimensional conformal blocks, we provide a rigorous\\nlink between the asymptotics of the spectral density of exchanged primaries and\\nthe leading singularity in the crossed channel. Our result has a direct\\napplication to systems of SL(2,R)-invariant correlators (also known as 1d\\nCFTs). It also puts on solid ground a part of the lightcone bootstrap analysis\\nof the spectrum of operators of high spin and bounded twist in CFTs in d>2. In\\naddition, a similar argument controls the spectral density asymptotics in large\\nN gauge theories.\\n']. \n",
      " Authors : ['Jiaxin Qiao, Slava Rychkov']. \n",
      " Arxiv ID : ['1709.00008'] \n",
      " \n",
      " \n",
      "\n",
      " 14) With TFIDF score 0.22555306553840637 and GPT-2 rank 29 \n",
      " Paper title : ['Operator Product Expansion and Conservation Laws in Non-Relativistic\\n  Conformal Field Theories']. \n",
      " Abstract : ['  We explore the consequences of conformal symmetry for the operator product\\nexpansions in nonrelativistic field theories. Similar to the relativistic case,\\nthe OPE coefficients of descendants are related to that of the primary.\\nHowever, unlike relativistic CFTs the 3-point function of primaries is not\\ncompletely specified by conformal symmetry. Here, we show that the 3-point\\nfunction between operators with nonzero particle number, where (at least) one\\noperator has the lowest dimension allowed by unitarity, is determined up to a\\nnumerical coefficient. We also look at the structure of the family tree of\\nprimaries with zero particle number and discuss the presence of conservation\\nlaws in this sector.\\n']. \n",
      " Authors : ['Siavash Golkar and Dam T. Son']. \n",
      " Arxiv ID : ['1408.3629'] \n",
      " \n",
      " \n",
      "\n",
      " 15) With TFIDF score 0.26968204975128174 and GPT-2 rank 30 \n",
      " Paper title : ['On level crossing in conformal field theories']. \n",
      " Abstract : ['  We study the properties of operators in a unitary conformal field theory\\nwhose scaling dimensions approach each other for some values of the parameters\\nand satisfy von Neumann-Wigner non-crossing rule. We argue that the scaling\\ndimensions of such operators and their OPE coefficients have a universal\\nscaling behavior in the vicinity of the crossing point. We demonstrate that the\\nobtained relations are in a good agreement with the known examples of the\\nlevel-crossing phenomenon in maximally supersymmetric $\\\\mathcal N=4$ Yang-Mills\\ntheory, three-dimensional conformal field theories and QCD.\\n']. \n",
      " Authors : ['G.P. Korchemsky']. \n",
      " Arxiv ID : ['1512.05362'] \n",
      " \n",
      " \n",
      "\n",
      " 16) With TFIDF score 0.2858161926269531 and GPT-2 rank 37 \n",
      " Paper title : ['Generalized Additivity in Unitary Conformal Field Theories']. \n",
      " Abstract : [\"  It was demonstrated in recent work that $d=4$ unitary CFT's satisfy a special\\nproperty: if a scalar operator with conformal dimension $\\\\Delta$ exists in the\\noperator spectrum, then the conformal bootstrap demands that large spin primary\\noperators have to exist in the operator spectrum of the CFT with a conformal\\ntwist close to $2\\\\Delta+2N$ for any integer $N$. In this paper the conformal\\nbootstrap methods that were used to find the anomalous dimension of the $N=0$\\noperators have been generalized to find the anomalous dimension of all large\\nspin operators of this class. In AdS these operators can be interpreted as the\\nexcited states of the product states of objects that were found in other works.\\n\"]. \n",
      " Authors : ['Gideon Vos']. \n",
      " Arxiv ID : ['1411.7941'] \n",
      " \n",
      " \n",
      "\n",
      " 17) With TFIDF score 0.38640448451042175 and GPT-2 rank 38 \n",
      " Paper title : ['Universal Constraints on Conformal Operator Dimensions']. \n",
      " Abstract : ['  We continue the study of model-independent constraints on the unitary\\nConformal Field Theories in 4-Dimensions, initiated in arXiv:0807.0004. Our\\nmain result is an improved upper bound on the dimension \\\\Delta of the leading\\nscalar operator appearing in the OPE of two identical scalars of dimension d.\\nIn the interval 1<d<1.7 this universal bound takes the form\\n\\\\Delta<2+0.7(d-1)^{1/2}+2.1(d-1)+0.43(d-1)^{3/2}. The proof is based on prime\\nprinciples of CFT: unitarity, crossing symmetry, OPE, and conformal block\\ndecomposition. We also discuss possible applications to particle phenomenology\\nand, via a 2-D analogue, to string theory.\\n']. \n",
      " Authors : ['Vyacheslav S. Rychkov, Alessandro Vichi']. \n",
      " Arxiv ID : ['0905.2211'] \n",
      " \n",
      " \n",
      "\n",
      " 18) With TFIDF score 0.357048362493515 and GPT-2 rank 46 \n",
      " Paper title : ['OPE inversion in Mellin space']. \n",
      " Abstract : ['  The fundamental ingredients that build the observables in conformal field\\ntheory are the spectrum of operators and the OPE coefficients, or equivalently,\\nthe two- and three-point functions of the theory. Recently an inversion formula\\nsolving the OPE coefficients by a convolution over the light-cone\\ndouble-discontinuities of the correlator has been found by Simon Caron-Huot.\\nTaking into account that the same OPE data determine the Mellin amplitude\\nrepresentation of the correlator, motivate us to look for an analogous\\ninversion formula in Mellin space, which we develops partially on this paper.\\n']. \n",
      " Authors : ['Carlos Cardona']. \n",
      " Arxiv ID : ['1803.05086'] \n",
      " \n",
      " \n",
      "\n",
      " 19) With TFIDF score 0.21329492330551147 and GPT-2 rank 51 \n",
      " Paper title : ['Bootstrapping the O(N) Archipelago']. \n",
      " Abstract : ['  We study 3d CFTs with an $O(N)$ global symmetry using the conformal bootstrap\\nfor a system of mixed correlators. Specifically, we consider all nonvanishing\\nscalar four-point functions containing the lowest dimension $O(N)$ vector\\n$\\\\phi_i$ and the lowest dimension $O(N)$ singlet $s$, assumed to be the only\\nrelevant operators in their symmetry representations. The constraints of\\ncrossing symmetry and unitarity for these four-point functions force the\\nscaling dimensions $(\\\\Delta_\\\\phi, \\\\Delta_s)$ to lie inside small islands. We\\nalso make rigorous determinations of current two-point functions in the $O(2)$\\nand $O(3)$ models, with applications to transport in condensed matter systems.\\n']. \n",
      " Authors : ['Filip Kos, David Poland, David Simmons-Duffin, Alessandro Vichi']. \n",
      " Arxiv ID : ['1504.07997'] \n",
      " \n",
      " \n",
      "\n",
      " 20) With TFIDF score 0.20328302681446075 and GPT-2 rank 54 \n",
      " Paper title : ['OPE Convergence in Conformal Field Theory']. \n",
      " Abstract : ['  We clarify questions related to the convergence of the OPE and conformal\\nblock decomposition in unitary Conformal Field Theories (for any number of\\nspacetime dimensions). In particular, we explain why these expansions are\\nconvergent in a finite region. We also show that the convergence is\\nexponentially fast, in the sense that the operators of dimension above Delta\\ncontribute to correlation functions at most exp(-a Delta). Here the constant\\na>0 depends on the positions of operator insertions and we compute it\\nexplicitly.\\n']. \n",
      " Authors : ['Duccio Pappadopulo, Slava Rychkov, Johnny Espin, Riccardo Rattazzi']. \n",
      " Arxiv ID : ['1208.6449'] \n",
      " \n",
      " \n",
      "\n",
      " 21) With TFIDF score 0.2305097132921219 and GPT-2 rank 58 \n",
      " Paper title : ['Conformal Regge Theory at Finite Boost']. \n",
      " Abstract : ['  The Operator Product Expansion is a useful tool to represent correlation\\nfunctions. In this note we extend Conformal Regge theory to provide an exact\\nOPE representation of Lorenzian four-point correlators in conformal field\\ntheory, valid even away from Regge limit. The representation extends\\nconvergence of the OPE by rewriting it as a double integral over continuous\\nspins and dimensions, and features a novel \"Regge block\". We test the formula\\nin the conformal fishnet theory, where exact results involving nontrivial Regge\\ntrajectories are available.\\n']. \n",
      " Authors : ['Simon Caron-Huot and Joshua Sandor']. \n",
      " Arxiv ID : ['2008.11759'] \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 22) With TFIDF score 0.36691975593566895 and GPT-2 rank 59 \n",
      " Paper title : ['The Lorentzian inversion formula and the spectrum of the 3d O(2) CFT']. \n",
      " Abstract : ['  We study the spectrum and OPE coefficients of the three-dimensional critical\\nO(2) model, using four-point functions of the leading scalars with charges 0,\\n1, and 2 ($s$, $\\\\phi$, and $t$). We obtain numerical predictions for low-twist\\nOPE data in several charge sectors using the extremal functional method. We\\ncompare the results to analytical estimates using the Lorentzian inversion\\nformula and a small amount of numerical input. We find agreement between the\\nanalytic and numerical predictions. We also give evidence that certain scalar\\noperators lie on double-twist Regge trajectories and obtain estimates for the\\nleading Regge intercepts of the O(2) model.\\n']. \n",
      " Authors : ['Junyu Liu, David Meltzer, David Poland, David Simmons-Duffin']. \n",
      " Arxiv ID : ['2007.07914'] \n",
      " \n",
      " \n",
      "\n",
      " 23) With TFIDF score 0.22024957835674286 and GPT-2 rank 64 \n",
      " Paper title : ['(2,0) Superconformal OPEs in D=6, Selection Rules and\\n  Non-renormalization Theorems']. \n",
      " Abstract : [\"  We analyse the OPE of any two 1/2 BPS operators of (2,0) SCFT$_6$ by\\nconstructing all possible three-point functions that they can form with\\nanother, in general long operator. Such three-point functions are uniquely\\ndetermined by superconformal symmetry. Selection rules are derived, which allow\\nus to infer ``non-renormalization theorems'' for an abstract superconformal\\nfield theory. The latter is supposedly related to the strong-coupling dynamics\\nof $N_c$ coincident M5 branes, dual, in the large-$N_c$ limit, to the bulk\\nM-theory compactified on AdS$_7 \\\\times$S$_4$. An interpretation of extremal and\\nnext-to-extremal correlators in terms of exchange of operators with protected\\nconformal dimension is given.\\n\"]. \n",
      " Authors : ['B. Eden, S. Ferrara and E. Sokatchev']. \n",
      " Arxiv ID : ['hep-th/0107084'] \n",
      " \n",
      " \n",
      "\n",
      " 24) With TFIDF score 0.3412710428237915 and GPT-2 rank 68 \n",
      " Paper title : ['Light cone OPE in a CFT with lowest twist scalar primary']. \n",
      " Abstract : ['  We study the operator product expansion (OPE) of two identical scalar primary\\noperators in the lightcone limit in a conformal field theory where a scalar is\\nthe operator with lowest twist. We see that in CFTs where both the stress\\ntensor and a scalar are the lowest twist operators, the stress tensor\\ncontributes at the leading order in the lightcone OPE and the scalar\\ncontributes at the subleading order. We also see that there does not exist a\\nscalar analogue of the average null energy condition (ANEC) for a CFT where a\\nscalar is the lowest twist operator.\\n']. \n",
      " Authors : ['Atanu Bhatta, Soham Ray']. \n",
      " Arxiv ID : ['1908.06303'] \n",
      " \n",
      " \n",
      "\n",
      " 25) With TFIDF score 0.38584864139556885 and GPT-2 rank 75 \n",
      " Paper title : ['Precision Islands in the Ising and $O(N)$ Models']. \n",
      " Abstract : ['  We make precise determinations of the leading scaling dimensions and operator\\nproduct expansion (OPE) coefficients in the 3d Ising, $O(2)$, and $O(3)$ models\\nfrom the conformal bootstrap with mixed correlators. We improve on previous\\nstudies by scanning over possible relative values of the leading OPE\\ncoefficients, which incorporates the physical information that there is only a\\nsingle operator at a given scaling dimension. The scaling dimensions and OPE\\ncoefficients obtained for the 3d Ising model, $(\\\\Delta_{\\\\sigma},\\n\\\\Delta_{\\\\epsilon},\\\\lambda_{\\\\sigma\\\\sigma\\\\epsilon},\\n\\\\lambda_{\\\\epsilon\\\\epsilon\\\\epsilon}) = (0.5181489(10), 1.412625(10),\\n1.0518537(41), 1.532435(19))$, give the most precise determinations of these\\nquantities to date.\\n']. \n",
      " Authors : ['Filip Kos, David Poland, David Simmons-Duffin, Alessandro Vichi']. \n",
      " Arxiv ID : ['1603.04436'] \n",
      " \n",
      " \n",
      "\n",
      " 26) With TFIDF score 0.2098252773284912 and GPT-2 rank 78 \n",
      " Paper title : ['Conformal fields and operator product expansion in critical quantum spin\\n  chains']. \n",
      " Abstract : ['  We propose a variational method for identifying lattice operators in a\\ncritical quantum spin chain with scaling operators in the underlying conformal\\nfield theory (CFT). In particular, this allows us to build a lattice version of\\nthe primary operators of the CFT, from which we can numerically estimate the\\noperator product expansion coefficients $C_{\\\\alpha\\\\beta\\\\gamma}^{\\\\textrm{\\nCFT}}$. We demonstrate the approach with the critical Ising quantum spin chain.\\n']. \n",
      " Authors : ['Yijian Zou, Ashley Milsted, Guifre Vidal']. \n",
      " Arxiv ID : ['1901.06439'] \n",
      " \n",
      " \n",
      "\n",
      " 27) With TFIDF score 0.24465230107307434 and GPT-2 rank 81 \n",
      " Paper title : ['Global Conformal Invariance and Bilocal Fields with Rational Correlation\\n  Functions']. \n",
      " Abstract : ['  The singular part of the \\\\textit{operator product expansion} (OPE) of a pair\\nof \\\\textit{globally conformal invariant} (GCI) scalar fields $\\\\phi$ of\\n(integer) dimension $d$ can be written as a sum of the 2-point function of\\n$\\\\phi$ and $d-1$ bilocal conformal fields $V_{\\\\nu}(x_1, x_2)$ of dimension\\n$(\\\\nu, \\\\nu)$, $\\\\nu = 1, ..., d-1$. As the correlation functions of $\\\\phi(x)$\\nare proven to be rational [6], we argue that the correlation functions of\\n$V_{\\\\nu}$ can also be assumed rational. Each $V_{\\\\nu}(x_1, x_2)$ is expanded\\ninto local symmetric tensor fields of \\\\textit{twist} (dimension minus rank)\\n$2\\\\nu$. The case $d=2$, considered previously [5], is briefly reviewed and\\ncurrent work on the $d=4$ case (of a Lagrangean density in 4 space--time\\ndimensions) is previewed.\\n']. \n",
      " Authors : ['Nikolay M. Nikolov, Yassen S. Stanev, Ivan T. Todorov']. \n",
      " Arxiv ID : ['hep-th/0211106'] \n",
      " \n",
      " \n",
      "\n",
      " 28) With TFIDF score 0.2342590093612671 and GPT-2 rank 89 \n",
      " Paper title : ['Anomalous Dimensions in the WF O($N$) Model with a Monodromy Line Defect']. \n",
      " Abstract : [\"  Implications of inserting a conformal, monodromy line defect in three\\ndimensional O($N$) models are studied. We consider then the WF O($N$) model,\\nand study the two-point Green's function for bulk-local fields found from both\\nthe bulk-defect expansion and Feynman diagrams. This yields the anomalous\\ndimensions for bulk- and defect-local primaries as well as one of the OPE\\ncoefficients as $\\\\epsilon$-expansions to the first loop order. As a check on\\nour results, we study the $(\\\\phi^k)^2{\\\\phi}^j$ operator both using the\\nbulk-defect expansion as well as the equations of motion.\\n\"]. \n",
      " Authors : ['Alexander S\\\\\"oderberg']. \n",
      " Arxiv ID : ['1706.02414'] \n",
      " \n",
      " \n",
      "\n",
      " 29) With TFIDF score 0.38660210371017456 and GPT-2 rank 92 \n",
      " Paper title : ['Correlation functions of three heavy operators - the AdS contribution']. \n",
      " Abstract : ['  We consider operators in N=4 SYM theory which are dual, at strong coupling,\\nto classical strings rotating in S^5. Three point correlation functions of such\\noperators factorize into a universal contribution coming from the AdS part of\\nthe string sigma model and a state-dependent S^5 contribution. Consequently a\\nsimilar factorization arises for the OPE coefficients. In this paper we\\nevaluate the AdS universal factor of the OPE coefficients which is explicitly\\nexpressed just in terms of the anomalous dimensions of the three operators.\\n']. \n",
      " Authors : ['Romuald A. Janik, Andrzej Wereszczynski']. \n",
      " Arxiv ID : ['1109.6262'] \n",
      " \n",
      " \n",
      "\n",
      " 30) With TFIDF score 0.2210056483745575 and GPT-2 rank 104 \n",
      " Paper title : ['Entanglement entropy of two disjoint intervals and the recursion formula\\n  for conformal blocks']. \n",
      " Abstract : ['  We reconsider the computation of the entanglement entropy of two disjoint\\nintervals in a (1+1) dimensional conformal field theory by conformal block\\nexpansion of the 4-point correlation function of twist fields. We show that\\naccurate results may be obtained by taking into account several terms in the\\noperator product expansion (OPE) of twist fields and by iterating the\\nZamolodchikov recursion formula for each conformal block. We perform a detailed\\nanalysis for the Ising conformal field theory and for the free compactified\\nboson. Each term in the conformal block expansion can be easily analytically\\ncontinued and so this approach also provides a good approximation for the von\\nNeumann entropy.\\n']. \n",
      " Authors : ['Paola Ruggiero, Erik Tonni, Pasquale Calabrese']. \n",
      " Arxiv ID : ['1805.05975'] \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "fastlstmrecommender(\"1212.3616\") #Jared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastlstmrecommender(\"1606.01857\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1) With TFIDF score 0.23111343383789062 and GPT-2 rank 0 \n",
      " Paper title : ['Can Hawking temperatures be negative ?']. \n",
      " Abstract : [\"  It has been widely believed that the Hawking temperature for a black hole is\\n$uniquely$ determined by its metric and $positive$. But, I find that this does\\n``not'' seem to be true in the recently discovered black holes which include\\nthe exotic black holes and the black holes in the three-dimensional higher\\ncurvature gravities. I show that the Hawking temperatures, which are measured\\nby the quantum fields in thermal equilibrium with the black holes, are $not$\\nthe usual Hawking temperature but the $new$ temperatures that have been\\nproposed recently and can be $negative$. The associated new entropy formulae,\\nwhich are defined by the first law of thermodynamics, versus the black hole\\nmasses show some genuine effects of the black holes which do not occur in the\\nspin systems. Some cosmological implications and physical origin of the\\ndiscrepancy with the standard analysis are noted also.\\n\"]. \n",
      " Authors : ['Mu-in Park']. \n",
      " Arxiv ID : ['hep-th/0610140'] \n",
      " \n",
      " \n",
      "\n",
      " 2) With TFIDF score 0.2363283336162567 and GPT-2 rank 1 \n",
      " Paper title : ['Information-carrying Hawking radiation and the number of microstate for\\n  a black hole']. \n",
      " Abstract : ['  We present a necessary and sufficient condition to falsify whether a Hawking\\nradiation spectrum indicates unitary emission process or not from the\\nperspective of information theory. With this condition, we show the precise\\nvalues of Bekenstein-Hawking entropies for Schwarzschild black holes and\\nReissner-Nordstr\\\\\"om black holes can be calculated by counting the microstates\\nof their Hawking radiations. In particular, for the extremal\\nReissner-Nordstr\\\\\"om black hole, its number of microstate and the corresponding\\nentropy we obtain are found to be consistent with the string theory results.\\nOur finding helps to refute the dispute about the Bekenstein-Hawking entropy of\\nextremal black holes in the semiclassical limit.\\n']. \n",
      " Authors : ['Qing-yu Cai, Chang-pu Sun and Li You']. \n",
      " Arxiv ID : ['1608.08304'] \n",
      " \n",
      " \n",
      "\n",
      " 3) With TFIDF score 0.32271698117256165 and GPT-2 rank 2 \n",
      " Paper title : ['Entanglement islands in higher dimensions']. \n",
      " Abstract : ['  It has been suggested in recent work that the Page curve of Hawking radiation\\ncan be recovered using computations in semi-classical gravity provided one\\nallows for \"islands\" in the gravity region of quantum systems coupled to\\ngravity. The explicit computations so far have been restricted to black holes\\nin two-dimensional Jackiw-Teitelboim gravity. In this note, we numerically\\nconstruct a five-dimensional asymptotically AdS geometry whose boundary\\nrealizes a four-dimensional Hartle-Hawking state on an eternal AdS black hole\\nin equilibrium with a bath. We also numerically find two types of extremal\\nsurfaces: ones that correspond to having or not having an island. The version\\nof the information paradox involving the eternal black hole exists in this\\nsetup, and it is avoided by the presence of islands. Thus, recent computations\\nexhibiting islands in two-dimensional gravity generalize to higher dimensions\\nas well.\\n']. \n",
      " Authors : ['Ahmed Almheiri, Raghu Mahajan, and Jorge E. Santos']. \n",
      " Arxiv ID : ['1911.09666'] \n",
      " \n",
      " \n",
      "\n",
      " 4) With TFIDF score 0.3216114342212677 and GPT-2 rank 3 \n",
      " Paper title : ['Islands in Asymptotically Flat 2D Gravity']. \n",
      " Abstract : ['  The large-N limit of asymptotically flat two-dimensional dilaton gravity\\ncoupled to N free matter fields provides a useful toy model for semiclassical\\nblack holes and the information paradox. Analyses of the asymptotic information\\nflux as given by the entanglement entropy show that it follows the Hawking\\ncurve, indicating that information is destroyed in these models. Recently,\\nmotivated by developments in AdS/CFT, a semiclassical island rule for entropy\\nhas been proposed. We define and compute the island rule entropy for black hole\\nformation and evaporation in the large-N RST model of dilaton gravity and show\\nthat, in contrast, it follows the unitary Page curve. The relation of these two\\nobservations, and interesting properties of the dilaton gravity island rule,\\nare discussed.\\n']. \n",
      " Authors : ['Thomas Hartman, Edgar Shaghoulian, Andrew Strominger']. \n",
      " Arxiv ID : ['2004.13857'] \n",
      " \n",
      " \n",
      "\n",
      " 5) With TFIDF score 0.2527661621570587 and GPT-2 rank 5 \n",
      " Paper title : ['Holographic entanglement entropy beyond coherent states']. \n",
      " Abstract : ['  We study entanglement entropy for a class of states in quantum field theory\\nthat are entangled superpositions of coherent states with well-separated\\nsupports, analogous to Einstein-Podolsky-Rosen or Bell states. We calculate the\\ncontributions beyond the area law in a simple model. In the case of strongly\\ncoupled conformal field theories, we argue that these states are\\nholographically dual to superpositions of bulk geometries. We note that for\\nthese states one can use the Ryu-Takayanagi holographic entanglement entropy\\nformula to calculate some terms in the entanglement entropy, but that there can\\nbe additional O(N^2) contributions. We argue that this class of states includes\\nthose generated by local quenches and thus that these cannot be described by a\\nclassical dual geometry. These considerations may be important for more fine\\ngrained treatments of holographic thermalization.\\n']. \n",
      " Authors : ['Curtis T. Asplund']. \n",
      " Arxiv ID : ['1205.5036'] \n",
      " \n",
      " \n",
      "\n",
      " 6) With TFIDF score 0.24412740767002106 and GPT-2 rank 7 \n",
      " Paper title : ['n-partite information in Hawking radiation']. \n",
      " Abstract : ['  We study the entanglement among sequential Hawking radiations in the\\nParikh-Wilczek tunneling model of Schwarzschild black hole. We identify the\\npart of classical correlation and that of quantum entanglement in bipartite\\ninformation and point out its imitated relation to quantum gravity correction.\\nExplicit computation of n-partite information shows that it is positive\\n(negative) for even (odd) $n$, which happens to agree with the holographic\\ncomputation. The fact that entanglement in the mutual information grows with\\ntime mimics the second law of thermodynamics. Later we extend our study to the\\nAdS black hole and find the total mutual information which includes the\\nclassical correlation is sensible to the Hawking-Page phase transition.\\n']. \n",
      " Authors : ['Shogo Kuwakino, Wen-Yu Wen']. \n",
      " Arxiv ID : ['1412.0802'] \n",
      " \n",
      " \n",
      "\n",
      " 7) With TFIDF score 0.3500198721885681 and GPT-2 rank 9 \n",
      " Paper title : ['Entanglement entropy from surface terms in general relativity']. \n",
      " Abstract : ['  Entanglement entropy in local quantum field theories is typically ultraviolet\\ndivergent due to short distance effects in the neighbourhood of the entangling\\nregion. In the context of gauge/gravity duality, we show that surface terms in\\ngeneral relativity are able to capture this entanglement entropy. In\\nparticular, we demonstrate that for 1+1 dimensional CFTs at finite temperature\\nwhose gravity dual is the BTZ black hole, the Gibbons-Hawking-York term\\nprecisely reproduces the entanglement entropy which can be computed\\nindependently in the field theory.\\n']. \n",
      " Authors : ['Arpan Bhattacharyya, Aninda Sinha']. \n",
      " Arxiv ID : ['1305.3448'] \n",
      " \n",
      " \n",
      "\n",
      " 8) With TFIDF score 0.30746620893478394 and GPT-2 rank 10 \n",
      " Paper title : ['E_6 and the bipartite entanglement of three qutrits']. \n",
      " Abstract : [\"  Recent investigations have established an analogy between the entropy of\\nfour-dimensional supersymmetric black holes in string theory and entanglement\\nin quantum information theory. Examples include: (1) N=2 STU black holes and\\nthe tripartite entanglement of three qubits (2-state systems), where the common\\nsymmetry is [SL(2)]^3 and (2) N=8 black holes and the tripartite entanglement\\nof seven qubits where the common symmetry is E_7 which contains [SL(2)]^7. Here\\nwe present another example: N=8 black holes (or black strings) in five\\ndimensions and the bipartite entanglement of three qutrits (3-state systems),\\nwhere the common symmetry is E_6 which contains [SL(3)]^3. Both the black hole\\n(or black string) entropy and the entanglement measure are provided by the\\nCartan cubic E_6 invariant. Similar analogies exist for ``magic'' N=2\\nsupergravity black holes in both four and five dimensions.\\n\"]. \n",
      " Authors : ['M. J. Duff and S. Ferrara']. \n",
      " Arxiv ID : ['0704.0507'] \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9) With TFIDF score 0.4030921459197998 and GPT-2 rank 11 \n",
      " Paper title : ['From qubits to black holes: entropy, entanglement and all that']. \n",
      " Abstract : ['  Entropy plays a crucial role in characterization of information and\\nentanglement, but it is not a scalar quantity and for many systems it is\\ndifferent for different relativistic observers. Loop quantum gravity predicts\\nthe Bekenstein-Hawking term for black hole entropy and logarithmic correction\\nto it. The latter originates in the entanglement between the pieces of spin\\nnetworks that describe black hole horizon. Entanglement between gravity and\\nmatter may restore the unitarity in the black hole evaporation process. If the\\ncollapsing matter is assumed to be initially in a pure state, then entropy of\\nthe Hawking radiation is exactly the created entanglement between matter and\\ngravity.\\n']. \n",
      " Authors : ['Daniel R. Terno']. \n",
      " Arxiv ID : ['gr-qc/0505068'] \n",
      " \n",
      " \n",
      "\n",
      " 10) With TFIDF score 0.2915576696395874 and GPT-2 rank 18 \n",
      " Paper title : ['Finiteness of Entanglement Entropy in Quantum Black Hole']. \n",
      " Abstract : ['  A logarithmic but divergent term usually appears in the computation of\\nentanglement entropy circumferencing a black hole, while the leading quantum\\ncorrection to the Bekenstein-Hawking entropy also takes the logarithmic form. A\\nquench model of CFT within finite Euclidean time was proposed in the\\n\\\\cite{Kuwakino:2014nra} to regard this logarithmic term as entanglement between\\nradiation and the black hole, and this proposal was justified by the\\nalternative sign for $n$-partite quantum information. However, this preliminary\\nform suffers from the notorious divergence at its low temperature limit. In\\nthis letter, we propose a modified form for black hole entanglement entropy\\nsuch that the divergence sickness can be cured. We discuss the final stage of\\nblack hole due to this modification and its relation to R{\\\\`e}nyi entropy,\\nhigher loop quantum correction and higher spin black holes.\\n']. \n",
      " Authors : ['Wen-Yu Wen']. \n",
      " Arxiv ID : ['1502.00583'] \n",
      " \n",
      " \n",
      "\n",
      " 11) With TFIDF score 0.26480722427368164 and GPT-2 rank 19 \n",
      " Paper title : ['On Hawking radiation from black rings']. \n",
      " Abstract : ['  We calculate the quantum radiation from the five dimensional charged rotating\\nblack rings by demanding the radiation to eliminate the possible anomalies on\\nthe horizons. It is shown that the temperature, energy flux and\\nangular-momentum flux exactly coincide with those of the Hawking radiation. The\\nblack rings considered in this paper contain the Myers-Perry black hole as a\\nlimit and the quantum radiation for this black hole, obtained in the\\nliterature, is recovered in the limit. The results support the picture that the\\nHawking radiation can be regarded as the anomaly eliminator on horizons and\\nsuggest its general applicability to the higher-dimensional black holes\\ndiscovered recently.\\n']. \n",
      " Authors : ['Umpei Miyamoto and Keiju Murata']. \n",
      " Arxiv ID : ['0705.3150'] \n",
      " \n",
      " \n",
      "\n",
      " 12) With TFIDF score 0.2416544258594513 and GPT-2 rank 20 \n",
      " Paper title : ['Towards a Bit Threads Derivation of Holographic Entanglement of\\n  Purification']. \n",
      " Abstract : ['  We apply the bit thread formulation of holographic entanglement entropy to\\nreduced states describing only the geometry contained within an entanglement\\nwedge. We argue that a certain optimized bit thread configuration, which we\\nconstruct, gives a purification of the reduced state to a full holographic\\nstate obeying a precise set of conditional mutual information relations. When\\nthis purification exists, we establish, under certain assumptions, the\\nconjectured $E_P = E_W$ relation equating the entanglement of purification with\\nthe area of the minimal cross section partitioning the bulk entanglement wedge.\\nAlong the way, we comment on minimal purifications of holographic states,\\ngeometric purifications, and black hole geometries.\\n']. \n",
      " Authors : ['Ning Bao, Aidan Chatwin-Davies, Jason Pollack, Grant N. Remmen']. \n",
      " Arxiv ID : ['1905.04317'] \n",
      " \n",
      " \n",
      "\n",
      " 13) With TFIDF score 0.24150198698043823 and GPT-2 rank 21 \n",
      " Paper title : ['Hawking Radiation from Rotating Black Holes and Gravitational Anomalies']. \n",
      " Abstract : ['  We study the Hawking radiation from Rotating black holes from gravitational\\nanomalies point of view. First, we show that the scalar field theory near the\\nKerr black hole horizon can be reduced to the 2-dimensional effective theory.\\nThen, following Robinson and Wilczek, we derive the Hawking flux by requiring\\nthe cancellation of gravitational anomalies. We also apply this method to\\nHawking radiation from higher dimensional Myers-Perry black holes. In the\\nAppendix, we present the trace anomaly derivation of Hawking radiation to argue\\nthe validity of the boundary condition at the horizon.\\n']. \n",
      " Authors : ['Keiju Murata, Jiro Soda']. \n",
      " Arxiv ID : ['hep-th/0606069'] \n",
      " \n",
      " \n",
      "\n",
      " 14) With TFIDF score 0.3799944818019867 and GPT-2 rank 22 \n",
      " Paper title : ['Noncommutativity and Holographic Entanglement Entropy']. \n",
      " Abstract : ['  In this paper we study the holographic entanglement entropy in a large N\\nnoncommutative gauge field theory with two $\\\\theta$ parameters by\\nRyu-Takayanagi prescription (RT-formula). We discuss what contributions the\\npresence of noncommutativity will make to the entanglement entropy in two\\ndifferent circumstances: 1) a rectangular strip and 2) a cylinder. Since we\\nwant to investigate the entanglement entropy only, we will not be discussing\\nthe finite temperature case in which the entropy calculated by the area of\\nminimal surface will largely be the thermal part rather than the entanglement\\npart. We find that divergence of the holographic entanglement entropy will be\\nworse in the presence of noncommutativity. In future study, we are going to\\nexplore the concrete way of computing holographic entanglement entropy in\\nhigher dimensional field theory and investigate more about the entanglement\\nentropy in the presence of black holes/black branes.\\n']. \n",
      " Authors : ['Tuo Jia, Zhaojie Xu']. \n",
      " Arxiv ID : ['1612.04857'] \n",
      " \n",
      " \n",
      "\n",
      " 15) With TFIDF score 0.3811575770378113 and GPT-2 rank 26 \n",
      " Paper title : ['Pulling Out the Island with Modular Flow']. \n",
      " Abstract : ['  Recent works have suggested that the entanglement wedge of Hawking radiation\\ncoming from an AdS black hole, will include an island inside the black hole\\ninterior after the Page time. In this paper, we propose a concrete way to\\nextract the information from the island by acting only on the radiation degrees\\nof freedom, building on the equivalence between the boundary and bulk modular\\nflow. We consider examples with black holes in JT gravity coupled to baths. In\\nthe case that the bulk conformal fields contain free massless fermion field, we\\nprovide explicit bulk picture of the information extraction process, where we\\nfind that one can almost pull out an operator from the island to the bath with\\nmodular flow.\\n']. \n",
      " Authors : ['Yiming Chen']. \n",
      " Arxiv ID : ['1912.02210'] \n",
      " \n",
      " \n",
      "\n",
      " 16) With TFIDF score 0.24478685855865479 and GPT-2 rank 28 \n",
      " Paper title : ['On black hole thermodynamics and the entropy function formalism']. \n",
      " Abstract : [\"  From the black hole thermodynamics point of view, we show that the entropy\\nfunction $\\\\mathbf{f}$ and the free energy $F$ are related via\\n$\\\\mathbf{f}=e_{I}q_{I}+\\\\Omega_Hq_I{A_{\\\\phi}^I}'-\\\\frac{\\\\partial F}{\\\\partial r}\\n|_{r_{H}}$. Assuming the entropy function is known for extremal black holes, we\\npropose an approach to calculate the entropy of non-extremal cases by slightly\\nmoving the extremal black hole geometry from extremality. The entropy of\\nnon-extremal $D1D5$- and $D1D5p$-branes in the presence of higher derivative\\ncorrections are computed as concrete examples. An attempt has also been made to\\nexplain why the entropy function method can calculate the corrected entropy\\nwithout knowing the exact form of black hole solution in higher derivative\\ngravity theories.\\n\"]. \n",
      " Authors : ['Xian-Hui Ge and Fu-Wen Shu']. \n",
      " Arxiv ID : ['0804.2123'] \n",
      " \n",
      " \n",
      "\n",
      " 17) With TFIDF score 0.2043505609035492 and GPT-2 rank 29 \n",
      " Paper title : ['Statistical Entropy of BTZ Black Hole in Higher Curvature Gravity']. \n",
      " Abstract : ['  For the BTZ black hole in the Einstein gravity, a statistical entropy has\\nbeen calculated to be equal to the Bekenstein-Hawking entropy. In this paper,\\nthe statistical entropy of the BTZ black hole in the higher curvature gravity\\nis calculated and shown to be equal to the one derived by using the Noether\\ncharge method. This suggests that the equivalence of the geometrical and\\nstatistical entropies of the black hole is retained in the general\\ndiffeomorphism invariant theories of gravity. A relation between the cosmic\\ncensorship conjecture and the unitarity of the conformal field theory on the\\nboundary of AdS is also discussed.\\n']. \n",
      " Authors : ['Hiromi Saida and Jiro Soda']. \n",
      " Arxiv ID : ['gr-qc/9909061'] \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 18) With TFIDF score 0.2801590859889984 and GPT-2 rank 31 \n",
      " Paper title : ['A new spin on entanglement entropy']. \n",
      " Abstract : ['  We argue that the usual notions of thermodynamic and entanglement entropy\\nhave novel analogs in the context of higher spin theories. In particular, the\\nWald and Ryu-Takayanagi formulas have natural higher spin extensions that we\\nwork out and study. On the CFT side, just as standard entanglement entropy in\\nCFT_2 can be computed from twist field correlators, we demonstrate that by\\nintroducing corresponding operators carrying higher spin charge we can\\nprecisely reproduce our results from the bulk.\\n  We also show that the first law for entanglement entropy implies the\\nlinearized field equations for the metric and higher spin fields, generalizing\\nrecent work on deriving the linearized Einstein equations from the first law.\\n']. \n",
      " Authors : ['Eliot Hijano and Per Kraus']. \n",
      " Arxiv ID : ['1406.1804'] \n",
      " \n",
      " \n",
      "\n",
      " 19) With TFIDF score 0.2314385026693344 and GPT-2 rank 32 \n",
      " Paper title : ['Black Hole Entropy from a Highly Excited Elementary String']. \n",
      " Abstract : ['  Suggested correspondence between a black hole and a highly excited elementary\\nstring is explored. Black hole entropy is calculated by computing the density\\nof states for an open excited string. We identify the square root of oscillator\\nnumber of the excited string with Rindler energy of black hole to obtain an\\nentropy formula which, not only agrees at the leading order with the\\nBekenstein-Hawking entropy, but also reproduces the logarithmic correction\\nobtained for black hole entropy in the quantum geometry framework. This\\nprovides an additional supporting evidence for correspondence between black\\nholes and strings.\\n']. \n",
      " Authors : ['Romesh K. Kaul']. \n",
      " Arxiv ID : ['hep-th/0302170'] \n",
      " \n",
      " \n",
      "\n",
      " 20) With TFIDF score 0.24423614144325256 and GPT-2 rank 34 \n",
      " Paper title : ['Holographic Holes in Higher Dimensions']. \n",
      " Abstract : [\"  We extend the holographic construction from AdS3 to higher dimensions. In\\nparticular, we show that the Bekenstein-Hawking entropy of codimension-two\\nsurfaces in the bulk with planar symmetry can be evaluated in terms of the\\n'differential entropy' in the boundary theory. The differential entropy is a\\ncertain quantity constructed from the entanglement entropies associated with a\\nfamily of regions covering a Cauchy surface in the boundary geometry. We\\ndemonstrate that a similar construction based on causal holographic information\\nfails in higher dimensions, as it typically yields divergent results. We also\\nshow that our construction extends to holographic backgrounds other than AdS\\nspacetime and can accommodate Lovelock theories of higher curvature gravity.\\n\"]. \n",
      " Authors : ['Robert C. Myers, Junjie Rao, Sotaro Sugishita']. \n",
      " Arxiv ID : ['1403.3416'] \n",
      " \n",
      " \n",
      "\n",
      " 21) With TFIDF score 0.3920353353023529 and GPT-2 rank 37 \n",
      " Paper title : ['Entanglement system, Casimir energy and black hole']. \n",
      " Abstract : ['  We investigate the connection between the entanglement system in Minkowski\\nspacetime and the black hole using the scaling analysis. Here we show that the\\nentanglement system satisfies the Bekenstein entropy bound. Even though the\\nentropies of two systems are the same form, the entanglement energy is\\ndifferent from the black hole energy. Introducing the Casimir energy of the\\nvacuum energy fluctuations rather than the entanglement energy, it shows a\\nfeature of the black hole energy. Hence the Casimir energy is more close to the\\nblack hole than the entanglement energy. Finally, we find that the entanglement\\nsystem behaves like the black hole if the gravitational effects are included\\nproperly.\\n']. \n",
      " Authors : ['Yun Soo Myung']. \n",
      " Arxiv ID : ['gr-qc/0511104'] \n",
      " \n",
      " \n",
      "\n",
      " 22) With TFIDF score 0.25218167901039124 and GPT-2 rank 39 \n",
      " Paper title : ['Strings and missing wormhole entanglement']. \n",
      " Abstract : ['  I show that holographic calculations of entanglement entropy in the context\\nof AdS bulk space modified by wormhole geometries provide the expected\\nentanglement magnitude. This arises in the context of string theory by means of\\nadditional geometric structure that is seen by the string in its bulk\\nevolution. The process can be described as a net entanglement flow towards\\nstringy geometry. I make use of the fact that as opposed to quantum field\\ntheory, strings have additional winding mode states around small extra\\ndimensions which modify the area computation given by the standard application\\nof the Ryu-Takayanagi entanglement entropy formula.\\n']. \n",
      " Authors : ['Andrei T. Patrascu']. \n",
      " Arxiv ID : ['1803.06164'] \n",
      " \n",
      " \n",
      "\n",
      " 23) With TFIDF score 0.28397443890571594 and GPT-2 rank 40 \n",
      " Paper title : ['Soft-Hair-Enhanced Entanglement Beyond Page Curves in a Black-hole\\n  Evaporation Qubit Model']. \n",
      " Abstract : ['  We propose a model with multiple qubits that reproduces the thermal\\nproperties of 4-dimensional (4-dim) Schwarzschild black holes (BHs) by\\nsimultaneously taking account of the emission of Hawking particles and the\\nzero-energy soft hair evaporation at horizon. The results verify that the\\nentanglement entropy between a qubit and other subsystems, including emitted\\nradiation, is much larger than the BH entropy analogue of the qubit, as opposed\\nto the Page curve prediction. Our result suggests that early Hawking radiation\\nis entangled with soft hair, and that late Hawking radiation can be highly\\nentangled with the degrees of freedom of BH, avoiding the emergence of a\\nfirewall at the horizon.\\n']. \n",
      " Authors : ['Masahiro Hotta, Yasusada Nambu and Koji Yamaguchi']. \n",
      " Arxiv ID : ['1706.07520'] \n",
      " \n",
      " \n",
      "\n",
      " 24) With TFIDF score 0.2839420735836029 and GPT-2 rank 43 \n",
      " Paper title : ['Information Flow in Black Hole Evaporation']. \n",
      " Abstract : ['  Recently, new holographic models of black hole evaporation have given fresh\\ninsights into the information paradox [arXiv:1905.08255, arXiv:1905.08762,\\narXiv:1908.10996]. In these models, the black hole evaporates into an auxiliary\\nbath space after a quantum quench, wherein the holographic theory and the bath\\nare joined. One particularly exciting development is the appearance of\\n\"ER=EPR\"-like wormholes in the (doubly) holographic model of\\n[arXiv:1908.10996]. At late times, the entanglement wedge of the bath includes\\nthe interior of the black hole. In this paper, we employ both numerical and\\nanalytic methods to study how information about the black hole interior is\\nencoded in the Hawking radiation. In particular, we systematically excise\\nintervals from the bath from the system and study the corresponding Page\\ntransition. Repeating this process ad infinitum, we end up with a fractal\\nstructure on which the black hole interior is encoded, implementing the\\nuberholography protocol of [arXiv:1612.00017].\\n']. \n",
      " Authors : ['Hong Zhe Chen, Zachary Fisher, Juan Hernandez, Robert C. Myers and\\n  Shan-Ming Ruan']. \n",
      " Arxiv ID : ['1911.03402'] \n",
      " \n",
      " \n",
      "\n",
      " 25) With TFIDF score 0.2798081934452057 and GPT-2 rank 46 \n",
      " Paper title : ['Decoding the Apparent Horizon: A Coarse-Grained Holographic Entropy']. \n",
      " Abstract : [\"  When a black hole forms from collapse in a holographic theory, the\\ninformation in the black hole interior remains encoded in the boundary. We\\nprove that the area of the black hole's apparent horizon is precisely the\\nentropy associated to coarse graining over the information in its interior,\\nsubject to knowing the exterior geometry. This is the maximum holographic\\nentanglement entropy that is compatible with all classical measurements\\nconducted outside of the apparent horizon. We identify the boundary dual to\\nthis entropy and explain why it obeys a Second Law of Thermodynamics.\\n\"]. \n",
      " Authors : ['Netta Engelhardt and Aron C. Wall']. \n",
      " Arxiv ID : ['1706.02038'] \n",
      " \n",
      " \n",
      "\n",
      " 26) With TFIDF score 0.28275445103645325 and GPT-2 rank 52 \n",
      " Paper title : ['A note on entanglement entropy and quantum geometry']. \n",
      " Abstract : ['  It has been argued that the entropy which one is computing in the isolated\\nhorizon framework of loop quantum gravity is closely related to the\\nentanglement entropy of the gravitational field and that the calculation\\nperformed is not restricted to horizons. We recall existing work on this issue\\nand explain how recent work on generalising these computations to arbitrary\\nspacetime dimensions D+1>2 supports this point of view and makes the duality\\nbetween entanglement entropy and the entropy computed from counting boundary\\nstates manifest. In a certain semiclassical regime in 3+1 dimensions, this\\nentropy is given by the Bekenstein-Hawking formula.\\n']. \n",
      " Authors : ['Norbert Bodendorfer']. \n",
      " Arxiv ID : ['1402.1038'] \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 27) With TFIDF score 0.27239707112312317 and GPT-2 rank 53 \n",
      " Paper title : ['Islands outside the horizon']. \n",
      " Abstract : ['  We consider an AdS$_2$ black hole in equilibrium with a bath, which we take\\nto have a dual description as (0+1)-dimensional quantum mechanical system\\ncoupled to a (1+1)-dimensional field theory serving as the bath. We compute the\\nentropies of both the quantum mechanical degrees of freedom and of the bath\\nseparately, while allowing contributions from entanglement wedge \"islands\". We\\nfind situations where the island extends {\\\\it outside} the black hole horizon.\\nThis suggests possible causality paradoxes which we show are avoided because of\\nthe quantum focusing conjecture. Finally, we formulate a version of the\\ninformation paradox for a black hole in contact with a bath in the\\nHartle-Hawking state, and demonstrate the role of islands in resolving this\\nparadox.\\n']. \n",
      " Authors : ['Ahmed Almheiri, Raghu Mahajan, Juan Maldacena']. \n",
      " Arxiv ID : ['1910.11077'] \n",
      " \n",
      " \n",
      "\n",
      " 28) With TFIDF score 0.2123071551322937 and GPT-2 rank 54 \n",
      " Paper title : ['On Non-Critical Superstring/Black Hole Transition']. \n",
      " Abstract : ['  An interesting case of string/black hole transition occurs in two-dimensional\\nnon-critical string theory dressed with a compact CFT. In these models the high\\nenergy densities of states of perturbative strings and black holes have the\\nsame leading behavior when the Hawking temperature of the black hole is equal\\nto the Hagedorn temperature of perturbative strings. We compare the first\\nsubleading terms in the black hole and closed string entropies in this setting\\nand argue that the entropy interpolates between these expressions as the energy\\nis varied. We compute the subleading correction to the black hole entropy for a\\nspecific simple model.\\n']. \n",
      " Authors : ['Andrei Parnachev, David A. Sahakyan']. \n",
      " Arxiv ID : ['hep-th/0512075'] \n",
      " \n",
      " \n",
      "\n",
      " 29) With TFIDF score 0.3674231767654419 and GPT-2 rank 58 \n",
      " Paper title : ['Entanglement entropy of two-dimensional Anti-de Sitter black holes']. \n",
      " Abstract : ['  Using the AdS/CFT correspondence we derive a formula for the entanglement\\nentropy of the anti-de Sitter black hole in two spacetime dimensions. The\\nleading term in the large black hole mass expansion of our formula reproduces\\nexactly the Bekenstein-Hawking entropy S_{BH}, whereas the subleading term\\nbehaves as ln S_{BH}. This subleading term has the universal form typical for\\nthe entanglement entropy of physical systems described by effective conformal\\nfields theories (e.g. one-dimensional statistical models at the critical\\npoint). The well-known form of the entanglement entropy for a two-dimensional\\nconformal field theory is obtained as analytic continuation of our result and\\nis related with the entanglement entropy of a black hole with negative mass.\\n']. \n",
      " Authors : ['Mariano Cadoni']. \n",
      " Arxiv ID : ['0704.0140'] \n",
      " \n",
      " \n",
      "\n",
      " 30) With TFIDF score 0.2326405644416809 and GPT-2 rank 59 \n",
      " Paper title : ['Hawking Radiation Energy and Entropy from a Bianchi-Smerlak\\n  Semiclassical Black Hole']. \n",
      " Abstract : ['  Eugenio Bianchi and Matteo Smerlak have found a relationship between the\\nHawking radiation energy and von Neumann entropy in a conformal field emitted\\nby a semiclassical two-dimensional black hole. We compare this relationship\\nwith what might be expected for unitary evolution of a quantum black hole in\\nfour and higher dimensions. If one neglects the expected increase in the\\nradiation entropy over the decrease in the black hole Bekenstein-Hawking A/4\\nentropy that arises from the scattering of the radiation by the barrier near\\nthe black hole, the relation works very well, except near the peak of the\\nradiation von Neumann entropy and near the final evaporation. These\\ndiscrepancies are calculated and discussed as tiny differences between a\\nsemiclassical treatment and a quantum gravity treatment.\\n']. \n",
      " Authors : ['Shohreh Abdolrahimi and Don N. Page']. \n",
      " Arxiv ID : ['1506.01018'] \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "fastlstmrecommender(\"1908.10996\") #Raghu Juan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally we end by mentioning the code for using the hidden states of generating abstract vectors. As mentioned before, this method does not lead to meaningful embeddings #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuffled.shape\n",
    "exp_size=1024\n",
    "def BatchGeneratortest(train_X_indices):\n",
    "    for i in range(1,132):\n",
    "        X_train = train_X_indices[exp_size*(i-1):exp_size*i,:]\n",
    "        yield (X_train)\n",
    "    X_train = train_X_indices[exp_size*i:-1,:]\n",
    "    yield (X_train)\n",
    "sentence=[]\n",
    "context=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for (X_train) in BatchGeneratortest(train_X_indices):\n",
    "    m = X_train.shape[0]\n",
    "    s0 = np.zeros((m, n_s))\n",
    "    c0 = np.zeros((m, n_s))\n",
    "    i=i+1\n",
    "    print(f\" \\n Metabatch number --------------------- {i} \\n \")\n",
    "    x=kerasmodel[1].predict([X_train, s0, c0])[1]\n",
    "    y=kerasmodel[1].predict([X_train, s0, c0])[2]\n",
    "    sentence.append(x)\n",
    "    context.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1=np.asarray(sentence[0:130])\n",
    "context1=np.asarray(context[0:130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_sentence_raw=np.asarray(sentence1).reshape(133120,12,64)\n",
    "reshaped_context_raw=context1.reshape(133120,12,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sentence_raw=np.average(reshaped_sentence_raw,axis=1)\n",
    "average_context_raw=np.average(reshaped_context_raw,axis=1)\n",
    "first_sentence_raw=reshaped_sentence_raw[:,0,:]\n",
    "first_context_raw=reshaped_context_raw[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iden=hep_abstracts_limit['id'].values\n",
    "iden[10343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"ModelsMar3/sentences_average_v2.txt\", \"w\")\n",
    "for i in range(133120):\n",
    "    f.write(f\"{iden[i]} \")\n",
    "    x=average_sentence_raw[i,:]\n",
    "    f.write((' '.join(['%0.10f']*x.size)+'\\n') % tuple(x))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"ModelsMar3/sentences_first_v2.txt\", \"w\")\n",
    "for i in range(133120):\n",
    "    f.write(f\"{iden[i]} \")\n",
    "    x=first_sentence_raw[i,:]\n",
    "    f.write((' '.join(['%0.10f']*x.size)+'\\n') % tuple(x))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below model uses the average of all the hidden states S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open(\"ModelsMar3/sentences_average_v2_gensim.txt\", \"w\")\n",
    "glove_file = datapath('/home/ubuntu/ModelsMar3/sentences_average_v2.txt')\n",
    "tmp_file = get_tmpfile(\"/home/ubuntu/ModelsMar3/sentences_average_v2_gensim.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "average_sentence_model = KeyedVectors.load_word2vec_format(tmp_file,)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sentence_model.wv.similar_by_word(\"1908.10996\",topn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below model uses the first hidden state S<1>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = open(\"ModelsMar3/sentences_first_v2_gensim.txt\", \"w\")\n",
    "glove_file = datapath('/home/ubuntu/ModelsMar3/sentences_first_v2.txt')\n",
    "tmp_file = get_tmpfile(\"/home/ubuntu/ModelsMar3/sentences_first_v2_gensim.txt\")\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "first_sentence_model = KeyedVectors.load_word2vec_format(tmp_file,)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence_model.wv.similar_by_word(\"1908.10996\",topn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No new code below. Only testing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.16314263373452945 \n",
      "Word2Vec distance : 0.43044283986091614 \n",
      "Our LSTM distance : 0.7871480584144592 \n"
     ]
    }
   ],
   "source": [
    "distance(\"holographi\",\"maldacena\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.3352357209886495 \n",
      "Word2Vec distance : 0.2427290976047516 \n",
      "Our LSTM distance : 0.8788047432899475 \n"
     ]
    }
   ],
   "source": [
    "distance(\"witten\",\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.14352409503724678 \n",
      "Word2Vec distance : 0.5671595335006714 \n",
      "Our LSTM distance : 0.8831663727760315 \n"
     ]
    }
   ],
   "source": [
    "distance(\"diagram\",\"feynman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.23661674825714057 \n",
      "Word2Vec distance : 0.11760114133358002 \n",
      "Our LSTM distance : 0.9001157283782959 \n"
     ]
    }
   ],
   "source": [
    "distance(\"brane\",\"witten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.2071838269895777 \n",
      "Word2Vec distance : 0.06880977749824524 \n",
      "Our LSTM distance : 0.14451353251934052 \n"
     ]
    }
   ],
   "source": [
    "distance(\"wilson\",\"rg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.10235812366783505 \n",
      "Word2Vec distance : 0.17221076786518097 \n",
      "Our LSTM distance : 0.9171896576881409 \n"
     ]
    }
   ],
   "source": [
    "distance(\"renormaliz\",\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.10621026902414282 \n",
      "Word2Vec distance : 0.22952578961849213 \n",
      "Our LSTM distance : -0.13791470229625702 \n"
     ]
    }
   ],
   "source": [
    "distance(\"graviti\",\"nonrenormaliz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.21725816887317384 \n",
      "Word2Vec distance : 0.37719425559043884 \n",
      "Our LSTM distance : 0.9705891609191895 \n"
     ]
    }
   ],
   "source": [
    "distance(\"pauli\",\"heisenberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.07691638769477205 \n",
      "Word2Vec distance : -0.1820271611213684 \n",
      "Our LSTM distance : 0.023427601903676987 \n"
     ]
    }
   ],
   "source": [
    "distance(\"isi\",\"fix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.03996431003842751 \n",
      "Word2Vec distance : 0.0982431173324585 \n",
      "Our LSTM distance : 0.9848122000694275 \n"
     ]
    }
   ],
   "source": [
    "distance(\"einstein\",\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.03008633381230795 \n",
      "Word2Vec distance : -0.02838786505162716 \n",
      "Our LSTM distance : 0.9894531965255737 \n"
     ]
    }
   ],
   "source": [
    "distance(\"graviti\",\"quantum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.09763714679433107 \n",
      "Word2Vec distance : -0.050817616283893585 \n",
      "Our LSTM distance : 0.9843711256980896 \n"
     ]
    }
   ],
   "source": [
    "distance(\"string\",\"graviti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.01920914502465594 \n",
      "Word2Vec distance : -0.12263837456703186 \n",
      "Our LSTM distance : 0.022416891530156136 \n"
     ]
    }
   ],
   "source": [
    "distance(\"isi\",\"conform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.001091574923656666 \n",
      "Word2Vec distance : -0.20761822164058685 \n",
      "Our LSTM distance : 0.030352793633937836 \n"
     ]
    }
   ],
   "source": [
    "distance(\"isi\",\"cft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.020424609488757274 \n",
      "Word2Vec distance : 0.42293426394462585 \n",
      "Our LSTM distance : 0.11947142332792282 \n"
     ]
    }
   ],
   "source": [
    "distance(\"mellin\",\"bootstrap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.1785067186563328 \n",
      "Word2Vec distance : 0.13560254871845245 \n",
      "Our LSTM distance : 0.9921423196792603 \n"
     ]
    }
   ],
   "source": [
    "distance(\"string\",\"theori\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.1708688062381955 \n",
      "Word2Vec distance : 0.16729344427585602 \n",
      "Our LSTM distance : 0.9938627481460571 \n"
     ]
    }
   ],
   "source": [
    "distance(\"quantum\",\"theori\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.07184288424688501 \n",
      "Word2Vec distance : 0.6671873927116394 \n",
      "Our LSTM distance : 0.8904553055763245 \n"
     ]
    }
   ],
   "source": [
    "distance(\"quark\",\"color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.09706513182803737 \n",
      "Word2Vec distance : 0.4093013107776642 \n",
      "Our LSTM distance : 0.9636445045471191 \n"
     ]
    }
   ],
   "source": [
    "distance(\"gaug\",\"confin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.09926400211216445 \n",
      "Word2Vec distance : 0.18497388064861298 \n",
      "Our LSTM distance : 0.9443263411521912 \n"
     ]
    }
   ],
   "source": [
    "distance(\"wilson\",\"confin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : 0.18325472491700956 \n",
      "Word2Vec distance : 0.6506814360618591 \n",
      "Our LSTM distance : 0.9688527584075928 \n"
     ]
    }
   ],
   "source": [
    "distance(\"quark\",\"confin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('lcr', 0.732690691947937),\n",
       " ('blason', 0.7310201525688171),\n",
       " ('simi', 0.7129940986633301),\n",
       " ('reanalysi', 0.7126487493515015),\n",
       " ('fp', 0.7118764519691467),\n",
       " ('depress', 0.7115901708602905),\n",
       " ('hom', 0.7081080675125122),\n",
       " ('lamm', 0.7070679068565369),\n",
       " ('reestim', 0.6944543719291687),\n",
       " ('supersymetri', 0.6909819841384888),\n",
       " ('intens', 0.6893722414970398),\n",
       " ('ressum', 0.6885448694229126),\n",
       " ('infraparticl', 0.6870040893554688),\n",
       " ('anisotropi', 0.6789699792861938),\n",
       " ('dslash', 0.6778581142425537)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model.wv.similar_by_word(\"conform\",topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.23661674825714057 \n",
      "Word2Vec distance : 0.11760114133358002 \n",
      "Our LSTM distance : 0.371532142162323 \n"
     ]
    }
   ],
   "source": [
    "pickledistance(\"witten\",\"brane\",weights6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.23661674825714057 \n",
      "Word2Vec distance : 0.11760114133358002 \n",
      "Our LSTM distance : -0.7255784273147583 \n"
     ]
    }
   ],
   "source": [
    "pickledistance(\"witten\",\"brane\",weights3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random distance : -0.23661674825714057 \n",
      "Word2Vec distance : 0.11760114133358002 \n",
      "Our LSTM distance : -0.5968130230903625 \n"
     ]
    }
   ],
   "source": [
    "pickledistance(\"witten\",\"brane\",weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_latest_p37]",
   "language": "python",
   "name": "conda-env-tensorflow2_latest_p37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
